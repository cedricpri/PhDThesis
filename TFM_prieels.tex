\documentclass[a4paper, 11pt]{report}
\usepackage[top=2.5cm, bottom=1.7cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{mathenv}
\usepackage{amsmath}
\usepackage{color}
\usepackage{caption}
\usepackage[bottom]{footmisc}
\usepackage{cancel}
\usepackage{multirow}
\usepackage[toc,page]{appendix}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\begin{document}

%Titlepage

\pagenumbering{roman}
\begin{titlepage}

	\centering
	\includegraphics[width=0.15\textwidth]{figs/image_UC.png}\par\vspace{1cm}
	{\scshape\LARGE Facultad de Ciencias \par}
	
	\vspace{1.5cm}
	
	%English title
	{\huge\bfseries Search for dark matter production in association with top quark pairs in the dilepton final state at $\sqrt{s} = $ 13 TeV\par}
	
	\vspace{0.6cm}
		
	%Spanish title
	{\LARGE (Búsqueda de materia oscura producida en asociación con un par de quarks top en el estado final dileptónico a $\sqrt{s} = $ 13 TeV) \par}
	
	\vspace{3cm}
	{\scshape\Large Trabajo de fin de Máster \\ para acceder al \par}
	\vspace{0.3cm}
	{\scshape\Large \textbf{MÁSTER EN FÍSICA, INSTRUMENTACIÓN \\ Y MEDIO AMBIENTE} \par}
	
	\begin{flushright}
	
	\vspace{3cm}
	{\Large Autor : Cédric PRIEËLS\par}
	{\Large Director : Jónatan PIEDRA GÓMEZ\\}
	{\Large Co-directora : Alicia CALDERÓN TAZÓN\\}
	\vspace{0.5cm}
	{\Large Junio 2017\par}
	\vfill
	
	\end{flushright}

\end{titlepage}

%Empty page

\clearpage
\thispagestyle{empty}
\phantom{a}
\vfill
\newpage

%Abstract and keywords

\setcounter{page}{1}

\section*{\huge{Abstract}}

\vspace {0.8cm}

%A search for dark matter production in association with a top-antitop quark pair is presented, in proton-proton collisions at a center of mass energy $\sqrt{s} = 13$ TeV collected with the CMS detector at the LHC. The analysis focuses on events with two opposite charge leptons and missing transverse energy. In order to discriminate the signal and the different backgrounds, a top-reconstruction technique is applied and is followed by a general Multi-Variate Analysis. No evidence of dark matter has been found, and upper limits on the dark matter production cross section are set in the context of scalar and pseudoscalar mediator dark matter models.

Dark matter production in proton-proton collisions at a center of mass energy $\sqrt{s} = 13$ TeV is searched for with the CMS experiment at the LHC, using a data sample corresponding to an integrated luminosity of 2.4 fb$^{-1}$, taken during 2016. This search is performed considering dark matter production in association with a pair of top quarks. It requires the presence of two leptons, jets and a large amount of missing transverse energy. 

We haven't managed to exclude any dark matter mediator production model by applying our analysis to the blinded dataset currently available to us, but we expect to be able to exclude the low dark matter mediator masses once we start studying the complete 2016 dataset.

\begin{center}
\textbf{Key words} : Particle Physics, CERN, CMS, dark matter, multi-variate analysis
\end{center}

\vspace {2.5cm}

\section*{\huge{Resumen}}

\vspace {0.8cm}
%Se busca materia oscura en asociación con un par de leptones top-antitop, en colisiones proton-proton a una energía en el centro de masa $\sqrt{s} = 13$ TeV en el detector CMS del LHC. El análisis está enfocado en sucesos que presentan dos leptones de carga opuesta y energía transversa faltante. Para discriminar la señal de los diferentes fondos, se aplica un método de top-reconstruction y un Multi-Variate Analysis. No evidencia de la existencia de materia oscura ha sido encontrado, y limites superiores en la sección eficaz de producción de materia oscura han sido calculados en el contexto de mediadores de tipo scalar y pseudoscalar. 

Se busca la producción de materia oscura en colisiones protón-protón a una energía en el centro de masa $\sqrt{s} = 13$ TeV con el experimento CMS del LHC, usando una muestra de datos que corresponde a una luminosidad integrada de 2.4 fb$^{-1}$, tomada en 2016. Esta búsqueda está hecha considerando producción de materia oscura en asociación con un par de quarks top y requiere la presencia de dos leptones, jets y una gran cantidad de energía transversa faltante.

No hemos podido excluir ningún modelo de producción de materia oscura aplicando nuestro análisis a los datos actualmente disponibles, pero esperamos poder excluir los mediadores de masa débiles cuando consideremos todos los datos de 2016.

\begin{center}
\textbf{Palabras claves} : Física de Partículas, CERN, CMS, materia oscura, análisis multivariable
\end{center}

\newpage

%Thank you notes

\section*{\huge{Agradecimientos}}

\vspace {0.8cm}

Primero, quiero dar las gracias a todo el grupo de Física de Altas Energías del Instituto de Física de Cantabria en general por haberme ofrecido un contrato de trabajo tan rápido y sin conocerme, que me ha permitido desarrollar mis conocimientos mientras estudiaba el Máster. Estos dos últimos años han sido una experiencia que nunca olvidaré y que me servirá seguramente mucho en el futuro. \\

Quiero agradecer a Jónatan en particular por su ayuda y sus consejos desde mi llegada al IFCA, por siempre tener la puerta abierta y por haber respondido con precisión y paciencia a todas las preguntas que tenía (y que no eran pocas). Gracias a Alicia también por toda su ayuda sobre algunos temas en particular de este trabajo, y a Rocío por su buen humor todos los días. Gracias también a Pablo por los consejos acertados dados antes de nuestras charlas importantes en MET+X, a Pupi por estar siempre accesible por Skype, y a Celso por su ayuda con el papeleo y la burocracia en general (que es incluso a veces peor que en Bélgica). \\ 

Muchas gracias también a todos los estudiantes con quien comparto el despacho. Me ha gustado vuestra compañía y me alegro de poder seguir trabajando con vosotros en los próximos años. Gracias en particular a Juan por haberme explicado con paciencia todas las diferentes partes de su análisis, y a Pedro por haber compartido conmigo todas las clases de Máster y todas estas horas comparando nuestros resultados para las diferentes prácticas. \\

Gracias también a todos mis amigos que me han acompañado a lo largo de mis estudios. Gracias a Nicolas en particular que me ha apoyado al principio de mis estudios, y a Alberto por haber compartido conmigo mi primer año aquí en el IFCA. Por supuesto, muchas gracias a Inés, porque todo este trabajo no hubiera sido posible sin su apoyo diario. \\

Por supuesto, gracias también a mis padres, a Antoine y Lucas por estar siempre presentes y ser un soporte imprescindible.

\newpage

%Table of contents

\tableofcontents

\thispagestyle{empty}
\newpage

%Here it starts!
\pagenumbering{arabic}

%Introduction

\chapter{Introduction}

Particle physics is the field which studies the matter surrounding us, along with the fundamental interactions between the particles. The origin of particle physics is to be found in the fifth century before JC, when Empedocles made the first attempt to divide matter into different categories (which were, according to him, Water, Earth, Fire and Air). A few years later Democritus added his own contribution to this newly born field, by saying that matter is made of indivisible entities that he decided to call atoms. This was a brilliant hypothesis, and over the next centuries, many different philosophers and scientists have joined their efforts and studied the composition of our Universe. In the 19th century, mainly thanks to the emergence of empirical reasoning, it was discovered that atoms are actually not fundamental particles, since it is possible to divide them into smaller entities, later identified as electrons, orbiting around a nucleus composed by of nucleons (protons and neutrons). Subatomic particle physics was starting to grow thanks to the development of quantum mechanics and quantum field theory  \cite{QFT}, and thanks to many experiments carried out during the 20th century. For example, detailed analysis of the data coming from the Stanford Linear Accelerator Center in the 1960s showed the presence of three scattering centers within the nucleons, which then can not be considered as elementary particles as previously thought \cite{Quarks}. Other particles were as well experimentally discovered during this period, such as the $J/\Psi$ particle which lead to the discovery of the charm quark in 1974. All of the discoveries made during the 20th century raised many questions, and the necessity of developing a complete model explaining all of these observations became obvious.

Nowadays, the model most accepted and used to describe the particles and the fundamental interactions between them is the so-called Standard Model, as seen in Figure \ref{fig:SM_wiki}. This model is simple in concept, but has been extremely good describing the phenomena observed so far, and made a lot of predictions that have now been proven to be true, such as the postulate of the Higgs mechanism \cite{HiggsPostulate} followed by the discovery of the Higgs boson in 2012 \cite{HiggsDiscovery1, HiggsDiscovery2}.  We currently know that there are 12 elementary particles in nature, divided into two categories (leptons and quarks) along with their 12 corresponding antiparticles. On the other side, the three main fundamental forces in particle physics can be described with four gauge bosons\footnote{A gauge boson is a particle carrying any of the fundamental forces. Elementary particles of the Standard Model can interact with each other by exchanging this kind of boson, as described by the so-called gauge theory.} (the gluon, the photon, and the W$^{\pm}$ and $Z^0$ bosons). The Higgs boson is the last piece of this model, needed to explain the origin of the mass of the particles. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=8cm, height=7cm]{figs/SM.png}
\caption{Summary of the Standard Model of elementary particles \cite{SM_wiki}.}
\label{fig:SM_wiki}
\end{center}
\end{figure}

However, as accurate as it is, this model is known to have several shortcomings which require further investigation. Over the next few years, eventual exotic particles which do not fit in the current model and which could be the sign of new physics were then extensively searched for. For example, in 1970, the first serious dark matter hypothesis was introduced because of gravitational anomalies observed by several astrophysicists, as a way to explain the apparent missing mass in the Universe. Indeed, the visible mass seemed to be way too low to explain several astrophysical processes, such as the rotation curves of the galaxies, as explained in Section \ref{sec:origin}. As far as we currently know from cosmological measurements, ordinary baryonic matter only constitutes around 5\% of the Universe, while dark matter represents around 27\% of the mass of the Universe (the rest is being considered as dark energy) \cite{Repartition}. 

%In this work are presented the latest methods and results used in particle physics to try to detect dark matter, using data coming from the LHC accelerator. The data analyzed in this work corresponds to one fifteenth of the complete 2016 dataset  and is coming from the Compact Muon Solenoid CMS detector, at CERN. This data has been taken at a center of mass energy of $\sqrt{s} = 13$ TeV and corresponds to an integrated luminosity of 2.4 fb$^{-1}$.

This document begins with a short theoretical introduction about the dark matter production process in general, and then in particular in the channel we are interested in (production of dark matter in association with a pair of top quarks in the dilepton final state), followed by some general explanations and descriptions of the detector used to record the data. The different datasets, samples and backgrounds of this analysis will then be presented, checked and studied in detail and the results obtained by performing a multi-variate analysis and using a neural network will be shown. Finally, the upper limits for the production cross section of different dark matter mediator masses will be plotted, to check if we observe or not any significative deviation between the observed and expected limits for the Standard Model.

%Theoretical introduction

\chapter{Theoretical introduction}

\section{At the origins of dark matter}\label{sec:origin}

The origins of the dark matter hypothesis can be traced back to the 17th and 18th centuries, shortly after Newton's works on gravitation \cite{Newton}. The dark matter considered back then was however quite different than the one considered nowadays, since it was thought to be ordinary matter which simply does not emit any kind of electromagnetic radiation (and is therefore invisible) but which has a strong impact in the gravitational point of view. Dark matter was for example considered during the 20th century to be found in massive astronomical objects able to absorb the light of other objects situated right behind them, such as black holes.

At the beginning of the 20th century, the first experimental evidences for the existence of dark matter were showed. In 1933, Fritz Zwicky determined the mass of the Coma Cluster using the virial theorem, which states that if we consider thousands of stars interacting gravitationally with each other, the average kinetic energy of any given star can be directly related to the average gravitational potential energy of this star. This means that the typical velocity of a star within the cluster can be related directly to the mass of the whole cluster. Thanks to this theorem, Zwicky was able to determine that the mass of the Coma Cluster should be around 400 to 500 times larger than the mass previously estimated by Edwin Hubble, who simply considered the number of visible galaxies in this cluster. Zwicky concluded that there should exist some kind of invisible matter that we can not detect but responsible for most of the mass of this cluster  \cite{Zwicky}.

Zwicky's results were controversial since they were based on statistical calculations relying on different hypothesis not always justified, such as the fact that the clusters must be gravitationally bound and they were actually proven to be overestimated later on \cite{Zwicky_off}. These results were soon followed by different observations leading to similar conclusions, the most famous one being the study of the observed and expected rotation curves of the galaxies in the 1970s \cite{Curves}. It was expected that the mass of the galaxies would be concentrated where more stars are visible, around the galactic center, and this would imply that outside of this high density region the velocities of the stars should decrease as the inverse of the square root of the distance, according to Kepler's laws. However, we now know that in reality this velocity has a flat behavior when this distance increases, as seen in Figure \ref{rotation_curve}. One of the easiest way to explain this kind of behavior is to introduce the concept of dark matter, which must not only be non-luminous as previously thought, but must also not absorb usual light either since we would have been able to detect this kind of matter already. Therefore, dark matter can not just be dark ordinary baryonic matter, it should be something different. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=8cm, height=6cm]{figs/RotationCurve2.jpeg}
\caption{Expected and observed rotation curves of the galaxy NGC 6503 \cite{Curves}. The black dots correspond to the data, and the line labeled as \textit{Luminous} corresponds to the expected rotation curve without dark matter.}
\label{rotation_curve}
\end{center}
\end{figure}

\newpage

Nowadays, many more experiments have contributed to this field and strengthen the dark matter hypothesis even though this is still not the only valid hypothesis accepted and under investigation (for example, another possibility is the MOdified Newtonian Dynamics hypothesis \cite{MOND}, modifying slightly newtonian's laws of gravity at large scales in order to explain some of the observations made by astrophysicists over the years).

The question remains to know the exact nature of dark matter particles. Astrophysicists have been able to determine the dark matter distribution within the galaxies from the latest sky surveys \cite{sky_surveys} and gravitational lensing studies \cite{lensing}, and we also know that not only dark matter does not interact at all with light, but it also does not interact with ordinary matter or itself. Some studies also showed that dark matter should be made of slow and cold particles (therefore, the neutrinos, which could have been considered as good candidates since they only interact weakly with ordinary matter can be rejected because they have such a low mass that they travel almost at the speed of light). Since we do not know any baryonic particle fulfilling all these conditions,  we can postulate the existence of new kinds of elementary particles to fill this role: the so-called MACHOs, the MAssive Compact Halo Object, or the WIMPs, the Weakly Interactive Massive Particles \cite{MACHO}. This latest group includes particles which are stable and are assumed to have been produced in pairs (particles, antiparticles) shortly after the Big Bang. This is typically the kind of particles we are hoping to find at the LHC.

\section{Dark matter production at the LHC}

Not only astrophysicists have their word to say when it comes to searching for dark matter, since particle physicists all around the globe hope that the LHC will help us find a way to determine if the dark matter hypothesis can be excluded or not \cite{ParticlePhysics}. Most physicists assume nowadays that dark matter was produced shortly after the Big Bang, along with ordinary matter, in a hot and dense Universe. If this assumption is correct, then the LHC is the perfect place to study this kind of particles, since its objective is to go back in time and study the Universe as it was just a fraction of a second after the Big Bang. For this reason, the LHC is a perfect tool to study the properties of dark matter, at least if we assume that it is actually made of particles and that it can actually be produced by LHC's proton-proton collisions, with a measurable cross section.

\newpage

The main idea is to search for missing transverse energy in the data coming from the accelerator, since the eventual dark matter particles produced are not expected to interact at all with the detector. This seems like a proper way to indirectly detect dark matter particles, but things may get complicated in practice because other Standard Model processes are also responsible for the apparition of missing transverse energy. For example, the detector resolution, production of neutrinos (which almost do not interact with ordinary matter) or particles escaping through some cracks of the detector are interpreted as missing energy as well and it is actually almost impossible to distinguish neutrinos from eventual dark matter particles. One of the ways we have to determine the kind of particle we are dealing with (besides the different kinematics or the angular distributions studies) is to estimate the fraction of proton-proton collisions that will produce a certain amount of missing transverse momentum, directly from the Standard Model and from theoretical Monte Carlo simulations. The main idea of the analysis consists in comparing these simulations for all the different expected backgrounds and the data obtained by the detector for different variables, in order to see if we observe an excess of data with respect to the simulations, which could be the sign of some new physics.

Different channels are expected to be good candidates for the eventual discovery of dark matter particles \cite{Production}. Usually, searches at the LHC are focused on considering production of a pair of WIMPs in association with initial or final state radiation (photons, gluons, W$^{\pm}$ or $Z^{0}$ bosons for example) in order for the detector to be able to trigger the measurement, since it is not able to know whether or not an event consisting of only two WIMPs particles happened. The basic idea for a typical dark matter search at LHC consists in looking for a high transverse momentum ($p_T$) particle production, in association with missing transverse momentum (MET, $\cancel{\textit{E}}_{T}$ or $E_T^{\text{miss}}$), corresponding to the imbalance of vector momentum in the plane perpendicular to the beam direction.

\section{The $t \bar t$ + DM dilepton channel}

Many different mechanisms of dark matter production are expected by different theories. This work will be focused the dark matter production in association with a top and an anti-top quarks, as showed in Figure \ref{feynman}. This process will be referred to as the $t \bar t$ + DM process throughout this work.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=6cm, height= 3.8cm]{figs/feynman2.jpg}
\caption{The channel studied in detail throughout this work: a pair of top quarks $t \bar t$ is sometimes expected to interact, producing a spin-0 mediator (dashed line in the diagram) and a pair of dark matter particle $\chi$ and antiparticle $\bar \chi$ \cite{Blog}.}
\label{feynman}
\end{center}
\end{figure}

A natural question which might arise is to know why we consider this channel with a pair of top quarks and missing transverse energy in the final state. The first reason is that the top quark interacts strongly, meaning that we can expect to produce a lot of $t \bar t$ at the LHC and even if the dark matter production cross section is really low, we still would expect to see at least a few interesting events. A second reason is that the top can be spotted quite easily within the huge amount of data produced at the LHC, since its signature can be well isolated with respect to the other backgrounds and processes. It is quite a natural process to first look where it is easier to find something. Another more fundamental reason is coming directly from the theory of the Standard Model. Some of the hypotheses about the nature of dark matter assume that it is a spin-0 particle, whose mediator should couple to ordinary particles in a similar way than the Higgs boson does. Thanks to last years data from the LHC, we actually know the the Higgs has a stronger coupling constant with the more massive particles, and the top quark has been found to be the most massive fermion of the Standard Model (way more massive than any other quark, actually) \cite{Top_mass}.

As stated previously, we currently do not have any way to directly detect any eventual dark matter particle since it is not supposed to interact at all with ordinary matter or itself. We then have to rely on the top quarks detection and on Monte Carlo simulations to be able to make any measurement in this channel. The top is actually not stable, has a lifetime of around $10^{-24}$ s, which way too short for us to be able to spot it directly \cite{Top} and decays almost instantly into a W$^{\pm}$ boson and a bottom quark through weak interaction. This is not quite over yet, since both these particles produced are not stable either, in the sense that the bottom quark immediately goes through a hadronisation process and produces a jet that we can detect, while the W boson can decay into two different channels: the leptonic one (W$^{\pm} \rightarrow \nu l$) and the hadronic one (W$^{\pm} \rightarrow q \bar q$). Since we have two W produced, it is possible to study three different channels: the dileptonic, the semileptonic and the hadronic channel. The channel studied throughout this work is the dileptonic one, because even though this is the channel having the smallest cross section for dark matter production, this is also the channel having the smallest number of significative background processes.

In summary, the channel studied results from the interaction between the protons in the colliding beams, which can sometimes produce a pair of top quarks and a pair of dark matter particle and antiparticle, through the apparition of a spin-0 mediator. We are then searching for dark matter production in the dilepton final state, since we know that around 30\% of the W$^{\pm}$ bosons decay into a lepton and a neutrino.

\section{Previous results}

A similar analysis has already been published by the CMS collaboration in August 2016 with data taken in 2015 at a center of mass energy of 13 TeV and corresponding to an integrated luminosity of 2.2 fb$^{-1}$ \cite{Previous}. As we can see in Figure \ref{previouslimits}, they didn't observe any significant deviation between the observed and expected limits (at least for a dark matter candidate having a 1 GeV mass, and for both scalar and pseudoscalar spin-0 mediators) with this limited luminosity. In this work, we expect to improve these results by adding more sophisticated techniques (such as an improved top reconstruction and a neural network) to the analysis and by exploiting more data.

\begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 180pt]{figs/previous_scalar.pdf}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 180pt]{figs/previous_pseudoscalar.pdf}
	}
   \end{minipage} \hfill
   \captionof{figure}{Expected and observed limits on the production cross section for an integrated luminosity of 2.2fb$^{-1}$, for both the scalar (left) and pseudoscalar (right) spin-0 mediators \cite{Previous}.}
   \label{previouslimits}

%LHC and the CMS detector

\chapter{The experimental device}

\section{The LHC collider}

The Large Hadron Collider is a circular underground proton-proton collider, situated at CERN, the European Organization for Nuclear Research, next to the city of Geneva. With its 27 kilometers of circumference, it is currently the most powerful accelerator in the world. A schematic representation of the accelerator can be found in Figure \ref{schematic_LHC}. This accelerator has been built by a collaboration of 22 countries in order to study and reproduce the Universe at its origin and the conditions right after the Big Bang, to make precision measurements to check the validity of the Standard Model and to search for exotic new physics \cite{Goals}.  

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=10cm, height=6cm]{figs/LHC.jpg}
\caption{Schematic representation of the LHC and its different detectors \cite{LHC}.}
\label{schematic_LHC}
\end{center}
\end{figure}

Two beams of protons can circulate in opposite directions at velocities close to the speed of light and collide in four different points of the accelerator, corresponding to the four detectors that have been designed to study the collisions: the Compact Muon Solenoid (CMS), A Toroidal LHC ApparatuS (ATLAS), A Large Ion Collider Experiment (ALICE) and LHCb. The LHC is currently able to accelerate beams made out of 2808 bunches of around $10^{11}$ protons up to a center of mass energy of 13 TeV, with an instantaneous luminosity of around $10^{34}$ cm$^{-2}$ s$^{-1}$ (these bunches are separated by 25 ns, leading to an impressive collision rate of 40 MHz). It is trivial to understand why having a higher energy is interesting from the physics point of view, since with a higher center of mass energy we would be able to create particles with higher masses. The luminosity is a crucial parameter as well, since a higher luminosity results in a higher number of collisions and an increase of the sensitivity. The integrated luminosity collected in the year 2016 corresponds to 35.9 fb$^{-1}$, which corresponds to around 30\% of the total luminosity expected during the first phase of operation of the accelerator, until the years 2018-2019 \cite{Plan}.

\section{The CMS detector}\label{subsubsec:CMS}

The data studied in this work has been taken by the CMS detector of the LHC, the Compact Muon Solenoid. This detector, along with ATLAS, has been designed to be a polyvalent detector, able to make measurements in most of the major different fields of particle physics (from precision measurements of Standard Model properties, to the Higgs hunting and to the search of exotic processes) \cite{CMS_general}. A schematic representation of the detector and its different layers can be found in Figure \ref{CMS}.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width= 10cm, height=7cm]{figs/CMS.png}
\caption{Schematic representation of the CMS detector \cite{CMSScheme}.}
\label{CMS}
\end{center}
\end{figure}

What is the exact meaning of the name CMS? First, this is a \textit{Compact} detector, which is really heavy (around 14000 tons) but all this weight is compacted into a relatively small volume, since the detector "only" has 15 meters of diameter and 28.7 meters of length. The \textit{Muon} part of the name comes from the fact that this is a detector specially dedicated to the detection and measurement of different properties of the muons, throughout a large range of energies. Finally, the \textit{Solenoid} part of the CMS name comes from the fact that its central piece is a huge solenoid able to produce a magnetic field of 3.8 T (equivalent to around 100 000 times the Earth's magnetic field) parallel to the beam direction, to curve and study different properties of the charged particles produced. \\

The detector is made out of different layers, each able to detect and measure different properties of the particles. The tracker is the center part of this detector: composed by pixels and microstrips of silicon, small electrical currents are produced by charged particles passing through this layer, giving a way to detect the exact position of the interaction and a way to reconstruct the exact track followed by the charged particles produced. It is really light, so that it does not affect the particles before we get a chance to measure their properties, and is extremely resistant to the radiation. The next layer is the electromagnetic calorimeter, made out of transparent lead-tungstate crystals to detect light produced by the electromagnetic showers produced when an electron or a photon comes crashing in this layer. Next comes the hadron calorimeter, able to measure the energy of most of the hadrons. Finally, the last layer, right after the solenoid, is made out of the muon detectors. This is the most external layer since muons interacts very little with ordinary matter except by ionization and scattering, and they are able to go through all the previous layers without getting affected. Of course, to avoid the presence of any instrumental missing energy coming from a particle escaping the detector without being detected, all the different layers of the detector must be completely hermetic.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=10cm, height=6cm]{figs/CMS_inside.png}
\caption{Layers of the CMS detector, and expected track for different kind of particles \cite{CMSInside}.}
\label{default}
\end{center}
\end{figure}

\newpage

The reconstruction and identification of the particles produced in a single event is a complicated task. This can be done however thanks to a powerful algorithm called the \textit{Particle Flow} \cite{PF}. With this algorithm and because of the acceptance of the detector, jets and electrons can be reconstructed up to a pseudorapidity\footnote{The pseudorapidity is a variable defined as $\eta = -ln\left(tan(\frac{\theta}{2})\right)$. This quantity is interesting because it is invariant under Lorentz transformations corresponding to boost along the z-axis.} $|\eta| < 3$ while muons can be reconstructed up to $|\eta| < 2.4$. \\

Since the LHC has an extremely high collision rate, a system of triggers is being used, in order for us to be able to keep potentially interesting events and reject the most common ones almost immediately, without having to store all the events, since it would be computationally impossible to keep all the events for the analysis. This system, known as the trigger system, is divided in two different categories: the L1 is the first trigger level, and is able to take extremely fast decisions based on a few basic variables only and using a dedicated hardware, to take the decision to keep or reject the event while the HLT (High Level Trigger) is more complex and slow, since it gathers information from different parts of the detector before reaching to a decision. At the end, a collision rate of around 1 kHz (out of the 40 MHz produced by the LHC) is being selected with this system to be stored and analyzed. \\

The most important magnitudes at CMS are those corresponding to the transverse plane. There is a simple reason for this: since the LHC is a proton-proton collider, the initial center of mass energy along the beam pipes (defined as the z-axis) is not known because the energy is distributed between the different quarks in unknown proportions. However, we do know that the initial transverse energy or initial transverse momentum is equal to exactly 0, and this is why we use this kind of variables. \\

%Signal and background processes

\chapter{Objects, datasets, triggers and samples}

In this chapter, the objects, datasets and triggers used for the analysis will be detailed, along with the expected signal and the different backgrounds coming in the way. All the major backgrounds of the analysis will be explained and detailed, along with the different methods developed to estimate their respective proportion.

\section{Objects and datasets}\label{sec:objects}

On one hand, the definitions of the different objects will be detailed in Section \ref{sec:event_recon} but basically, we apply some cuts on various variables (on the $p_T$ and on the isolation of the leptons between each other and between the jets produced, for example) to select only interesting leptons and achieve a particular signal efficiency. Different working points are then made available for us, to select the kind of lepton we are interested in for the analysis we want to perform, going from the so-called loose working point, for analyses with a clean final state to the tight working point, for analyses having a lot of backgrounds. A loose lepton is defined with only some basic requirements in order to keep a high efficiency (of the order of 90\% for electrons having a $p_T$ larger than 20 GeV) while the medium and the tight leptons are defined by applying cuts much more restrictive, resulting in better but less efficient objects (the efficiency is estimated to be of the order of respectively 80 and 70\% for the same kind of electrons). It is important to note that the cuts which go inside and define these working points depend on several factors, including the running conditions, meaning that they have to be updated and tuned again every time these conditions change. \\

As described previously, the datasets used have been recorded during the Run II at the LHC, at a center of mass energy of $\sqrt{s} = 13$ TeV and correspond to an integrated luminosity of 2.4 fb$^{-1}$ for the signal region, and 35.9 fb$^{-1}$ for the different control regions studied (the uncertainty on the value of these luminosities has been estimated by the luminosity monitoring group in \cite{lumi} to be equal to 2.5\%). The events were actually selected randomly from the 35.9 fb$^{-1}$ complete dataset to follow the blinding policy of the MET+X group at CERN\footnote{The blinding policy is a methodology that forces the analyzer to define all his procedures and signal region definitions using only a small subset of the data. This kind of blinding policy is applied to most of the searches for new physics, and it is quite easy to understand why such a policy is applied. When discovering or searching for a new process, we need the observation to be statistically significant by collecting more data, and we want to make sure to avoid any conscious or even subconscious bias when one tries to optimize its analysis based on what has already been seen \cite{blinding}.}. The acquisition of the data has been divided into small entities called runs and grouped into 8 different periods, going from 2016A to 2016H (but the run period 2016A has been taken with the magnet of CMS turned off and does not have any interest for this analysis, since we loose a lot of information about the different leptons produced), as described in Table \ref{tab:datasets}. Each period is then made out of different files corresponding to the five different interesting processes available for any dilepton analysis (SingleMuon, SingleElectron, DoubleMuon, DoubleEG and MuonEG), usually referred to as datasets. All of these datasets have been updated at the beginning of 2017 and correspond to the ReReco of the data (which consists mainly in updated calibrations and minor corrections of the data) \cite{Runs}. %Finally, it is important to note that since the LHC is colliding bunches and not single protons, we expect to observe many different events every time the beams cross. These multiple interactions happening every time are referred to as the \textit{pile-up}, and usually complicate the reconstruction of the event. The amount of pile-up is characterized directly by the number of primary vertices of the interaction.

\begin{center}
	\begin{tabular}{c|c|c|c}
		Era & From run & To run & Luminosity (fb$^{-1}$) \\
		\hline \hline
		Run2016B & 272007 & 275376 & 5,748 \\
		Run2016C & 275657 & 276283 & 2,573 \\
		Run2016D & 276315 & 276811 & 4,248 \\
		Run2016E & 276831 & 277420 & 4,009 \\
		Run2016F & 277772 & 278808 & 3,102 \\
		Run2016G & 278820 & 280385 & 7,540 \\
		Run2016H & 280919 & 284044 & 8,606 \\ \hline
		Total & 272007 & 284044 & 35,827 \\
	\end{tabular}
\end{center}
\captionof{table}{Different datasets taken so far during the Run II data period and luminosities associated, calculated with Brilcalc \cite{Runs, Brilcalc}.}
\label{tab:datasets}

\section{Triggers} \label{sec:triggers}

Since the LHC is producing an enormous amount of collisions and since it is currently not computationally possible to record all the events, a trigger system has been put in place, as described in Section \ref{subsubsec:CMS}. %This system allows us to store only the potentially interesting events, by studying extremely quickly some basic variables of the events, such as the $p_T$ of the leptons produced. 
The single and dilepton triggers used in this analysis are the following:

\begin{itemize}
\item {\bf SingleMuon} : HLT\_IsoTkMu22\_v* \\ and HLT\_IsoMu22\_v*
\item {\bf SingleElectron} : HLT\_Ele27\_eta2p1\_WPLoose\_Gsf\_v* \\ and HLT\_Ele45\_WPLoose\_Gsf\_v*
\item {\bf DoubleMuon} : HLT\_Mu17\_TrkIsoVVL\_Mu8\_TrkIsoVVL\_DZ\_v* \\ and HLT\_Mu17\_TrkIsoVVL\_TkMu8\_TrkIsoVVL\_DZ\_v*
\item {\bf DoubleEG} : HLT\_Ele23\_Ele12\_CaloIdL\_TrackIdL\_IsoVL\_DZ\_v
\item {\bf MuonEG} : HLT\_Mu8\_TrkIsoVVL\_Ele23\_CaloIdL\_TrackIdL\_IsoVL\_v* \\ and HLT\_Mu23\_TrkIsoVVL\_Ele12\_CaloIdL\_TrackIdL\_IsoVL\_v*
\end{itemize}

It is important to note that the single and dilepton triggers are combined together in most of the analyses in order to increase their efficiency to select signal events, and that the different numbers appearing in the names of the triggers correspond to the minimum $p_T$ of the lepton required to pass the corresponding trigger.

\section{Monte Carlo samples}

As previously explained, the expected signal and most of the major background processes are produced using Monte Carlo simulations, and have been simulated from different theoretical models \cite{MC_general}. All the MC samples and possible backgrounds considered for this analysis along with their respective cross sections and the total number of events generated are represented in Table \ref{tab:MC_samples}. These samples have been produced using different MC generators and have different levels of accuracy. For example, the $t \bar t$ process, our most important background, has been generated using POWHEG V2 \cite{Powhegv2} and the single top background that we will mention in Section \ref{sec:backgrounds} has been generated using POWHEG V1. Most of the backgrounds have been generated at the next to leading order (NLO) accuracy level and a few samples have even been generated at the next to next to leading order (NNLO). Events are then hadronized and showered using PYTHIA8 \cite{Pythia8}, and then interfaced to GEANT4 \cite{Geant4}, which contains a detailed model of CMS and the interactions of the particles with its subdetectors. \\

%The different backgrounds have been simulated thanks to different Monte Carlo generators. Even though this is not a highly relevant subject for this work, it is important to explain it a little bit, since they are usually considered as "black boxes" by the experimental physicists and because we are actually extremely dependent on the quality of these simulations to get quality results and limits on the eventual existence of dark matter production in this channel. A generator is a library which is able to, as its name seems to be indicating, generate high-energy particle physics events from different theoretical models.

\begin{table}[h]
\centering
%\resizebox{490pt} {!}{
\begin{tabular}{c|c|c|c}
& & & \\
Process & Accuracy & Cross section [pb] & Number of generated events \\
& & & \\
 \hline \hline
 & & & \\
Z $\rightarrow$ ll (low $m_{ll}$) & NLO & 18610   & 30899063  \\
Z $\rightarrow$ ll (high $m_{ll}$) & NLO & 6025.2  & 28751199  \\
W$\gamma \rightarrow$ l $\nu$ $\gamma$ & NNLO & 586     &  4316841  \\
Z$\gamma \rightarrow$ ll $\gamma$ & NLO & 131.3   &    14861  \\
$t \bar t \rightarrow$ ll $\nu$$\nu$ & NNLO & 87.31   &  4995600  \\
Single top & NNLO & 35.6  &  1000000  \\
Single antitop & NNLO & 35.6    &   999400  \\
WZ $\rightarrow$ lll $\nu$ & NLO & 4.42965 &  2000000  \\
ZZ $\rightarrow$ llll & NLO & 1.212   &  6669188  \\
ZZ $\rightarrow$ ll $\nu$$\nu$ & NLO & 0.564   &  8785050  \\
ZZ $\rightarrow$ ll $q \bar q$ & NLO & 3.22    & 15301695  \\
WZZ & NLO & 0.05565 &   249800  \\
$t \bar t$W $\rightarrow$ l $\nu$ & NLO & 0.2043  &   252673  \\
$t \bar t$W $\rightarrow$ $q \bar q$ & NLO & 0.4062  &   833298  \\
$t \bar t$Z $\rightarrow$ ll $\nu$$\nu$ & NLO & 0.2529  &   398600  \\
$t \bar t$Z $\rightarrow$ $q \bar q$ & NLO & 0.5297  &   749400  \\
WW $\rightarrow$ ll $\nu$$\nu$ & NNLO &12.178  &  1979988  \\
ggWW $\rightarrow$ ll $\nu$$\nu$ &  & 0.5905  &   500000  \\
ggH $\rightarrow$ WW $\rightarrow$ ll $\nu$$\nu$ &  & 0.9913  &   100000  \\
H $\rightarrow$ WW &  & 0.5686  &   994337  \\
$t \bar t$H $\rightarrow$ $b\bar b$ & NNLO &0.212   &  1501096  \\
W $\rightarrow$ l $\nu$ & NNLO & 61526.7 & 24156124  \\
& & & \\
\end{tabular}
%}
\caption{Monte Carlo samples used for this analysis along with their respective cross sections and number of events generated.}
\label{tab:MC_samples}
\end{table} 

\newpage

\section{Dark matter signal}

Since we do not know the mass and the type of the mediator of the interaction we are interested in, or the mass of the eventual dark matter particles produced, many different signal samples have been produced for different mass points, for us to compare all these possible signals to the data. These samples can be separated into two different categories (when the spin-0 mediator is considered to have either scalar or pseudoscalar couplings) and are represented in Tables \ref{scalar} and \ref{pseudoscalar}, where $m_{\chi}$ represents the mass of the dark matter particle and $m_{\phi}$ represents the mass of the mediator. \\ \vspace{10pt}

\begin{minipage}[c]{.48\linewidth}
   	\begin{center}
		\resizebox{200pt} {!}{
		\begin{tabular}{c|c|c|c|c|c|c|c}
			& \multicolumn{7}{c}{} \\
		 	$m_{\chi}$ [GeV] & \multicolumn{7}{c}{$m_S$ [GeV]}\\
			& \multicolumn{7}{c}{} \\
			\hline \hline
			& & & & & & & \\
			1 & 10 & 20 & 50 & 100 & 200 & 300 & 500 \\
			10 & 10 & 15 & 50 & 100 & & & \\
			50 & 10 & & 50 & 95 & 200 & 300 & \\
			& & & & & & & \\
		\end{tabular}
		} 
		\captionof{table}{Different signal samples available for the scalar mediator. \\} 
		\label{scalar}
	\end{center}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\begin{center}
		\resizebox{216pt} {!}{
		\begin{tabular}{c|c|c|c|c|c|c|c|c}
			& \multicolumn{7}{c}{} \\
		 	$m_{\chi}$ [GeV] & \multicolumn{8}{c}{$m_S$ [GeV]}\\
			& \multicolumn{7}{c}{} \\
			\hline \hline
			& & & & & & & \\
			1 & 10 & 20 & 50 & 100 & 200 & 300 & 500 & 1000 \\
			10 & 10 & 15 & 50 & 100 & & & & \\
			50 & 10 & & 50 & 95 & 200 & 300 & & \\
			& & & & & & & \\
		\end{tabular}
		} 
		\captionof{table}{Different signal samples available for the pseudoscalar mediator. \\} 
		\label{pseudoscalar}
	\end{center}
\end{minipage} \hfill  

\newpage

To produce all these samples, a series of assumptions has been made, following the recommendations of the LHC Dark Matter Forum \cite{DMForum}. First of all, the couplings between the mediator and the usual Standard Model and dark matter particles have been considered equal to one. Then, the dark matter particle is assumed to be a Dirac fermion (meaning that the dark matter particle is different than its corresponding antiparticle, and that this particle has a semi-integer spin) and finally, for the scalar model, no mixing with the Higgs boson is assumed. The different samples produced for the different mass points considered have of course different cross sections, as seen on Table \ref{tab:signal_xs}, where only the samples having a 1 GeV mediator are represented, since these are the only samples used for our analysis so far. All these signal samples have been produced by Monte Carlo simulations at the leading order level, using Madgraph.

\begin{table}[h]
\centering
%\resizebox{490pt} {!}{
\begin{tabular}{c|c|c|c}
& & & \\
$m_{\chi}$ [GeV] & $m_S$ [GeV] & Cross section [pb] & Number of generated events \\
& & & \\
 \hline \hline
& & & \\
\textbf{Scalar mediator} & & & \\
1 & 10 & 19.590   &  240379 \\
1 & 20 & 10.480   &  251225 \\
1 & 50 &  2.941   &  253308 \\
1 & 100 &  0.6723  &  257761 \\
1 & 200 &  0.009   &  254295 \\
1 & 500 &  0.005   &  250524 \\
& & & \\
\hline
& & & \\
\textbf{Pseudoscalar mediator} & & & \\
1 & 10 & 0.441   & 252054 \\
1 & 20 & 0.399   & 249253 \\
1 & 50 & 0.303   & 255516 \\
1 & 100 & 0.1909  & 249971 \\
1 & 200 & 0.0836  & 240536 \\
1 & 500 & 0.0054  & 241952 \\           
& & & \\
\end{tabular}
%}
\caption{Signal samples studied in this work at several grid points, along with their
corresponding production cross sections from Madgraph (at leading order) for both scalar
and pseudoscalar mediator models, and number of generated events.}
\label{tab:signal_xs}
\end{table}                                                                                         

\section{Major Standard Model backgrounds} \label{sec:backgrounds}

Many backgrounds appear in this analysis and it is crucial to study them in detail if we want to be able to extract any information from the data, in order to be able to detect any eventual excess between this data and the different backgrounds simulated. We expect however to be able to reduce strongly the backgrounds of the analysis by applying cuts in different variables, but some of them, characterized as irreducible backgrounds, present similar kinematic distributions as our signal does, making them therefore impossible to remove completely. The major backgrounds of this analysis are detailed in the following bullets, while the actual methods or variables available to reduce them will be studied in Section \ref{sec:discriminating_var}.

%The main way to distinguish between the signal and this background can be the presence of missing transverse energy, which might be the sign of the presence of some dark matter production. Applying a cut in order to remove the events which do not have an high amount of MET is then a great way to reduce this background but does not allow us to remove completely this background since, as explained previously, dark matter is not the only way to produce this king of missing energy (particles escaping through some cracks of the detector or neutrinos are also responsible for apparition of some MET).

\begin{itemize}
\item \textbf{$t \bar t$ production}. This background, which can be produced through different channels as represented in Figure \ref{ttFeynman}, is the one presenting the most similar signature as the one expected for the $t \bar t$ + DM process we are looking for, since both are characterized by two top quarks in the final state and therefore, the biggest challenge of this analysis consists in finding a way to separate the $t \bar t$ production from our signal. This background is characterized as irreducible since its main kinematic distributions are really similar to the ones expected for the signal, especially for the low mediator mass points.

%The main way to do so is by introducing the $m_{T2}^{ll}$ variable\footnote{The definition and the use of this variable will be detailed in the section \ref{sec:signalextraction}.} (as we will see later on, this variable is actually one of the most important variable of the analysis since it provides a strong discrimination between the signal and the different backgrounds, giving us a perfect cut to apply to reduce the backgrounds as much as possible while leaving the signal as it is).

\begin{minipage}[c]{.24\linewidth}
   	\centering{
		\includegraphics[width= 80pt, height= 70pt]{figs/tt1.jpg} \\
	}
   \end{minipage}
   \begin{minipage}[c]{.24\linewidth}
   	\centering{
		\includegraphics[width= 80pt, height= 70pt]{figs/tt2.jpg} \\
	}
\end{minipage}
	 \begin{minipage}[c]{.24\linewidth}
   	\centering{
		\includegraphics[width= 80pt, height= 70pt]{figs/tt3.jpg} \\
	}
\end{minipage}
	\begin{minipage}[c]{.24\linewidth}
   	\centering{
		\includegraphics[width= 80pt, height= 70pt]{figs/tt4.jpg} \\
	}
\end{minipage}
\begin{center}
\captionof{figure}{Different leading order Feynman diagrams for the production of $t \bar t$ \cite{tt}.}
\label{ttFeynman}
\end{center}

\item \textbf{tW$^{\pm}$}. This process is the second most important background in our analysis at the final selection level, because the cross section of this process is  much higher than the one expected for the signal (cf. Tables \ref{tab:MC_samples} and \ref{tab:signal_xs}). It is an important background because it features a single top in the final state, and because one out of every three W bosons produced decay leptonically into a lepton and a neutrino, which is responsible for the presence of some missing transverse energy. The main way to reduce this background is to ask for the events to present at least two jets, coming from the production of two quarks: since our signal has two quarks in the final state, it should then pass this requirement, while this background only features one jet and should be reduced with this cut. 

\item \textbf{ttW$^{\pm}$ and ttZ$^{0}$}. Having a much lower cross section, we expect this kind of background to have a lower impact on the final results than the previous ones, even though it has a pair of top quarks in the final state, just as our signal does. It can be divided into four parts, corresponding to the main different decay modes of the W and the Z bosons, as shown in Figure \ref{ttV_plot}. However, two of these four decay modes can be strongly reduced by introducing the $m_{T2}^{ll}$ variable\footnote{While the definition and the use of this variable will actually be detailed in the section \ref{sec:discriminating_var}, it is already important to know that this variable is really important, because it presents a strong discriminating power between the signal and the backgrounds in our analysis.} in our analysis. As we can see on the right plot in this figure, when applying the $m_{T2}^{ll} > 80$ GeV cut of our analysis, only two contributions are important: the ttW when the W decays to one lepton and one neutrino (49\%), and the ttZ background when the Z decays to two neutrinos (34 \%). The other two decay modes can at first order be neglected in this analysis, since they seem to be reducible backgrounds.
\end{itemize}

   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width=7cm, height=7cm]{figs/ttv-0-2.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width=7cm, height=7cm]{figs/ttv-80-2.png}
	}
   \end{minipage} \hfill
\captionof{figure}{Proportion of each decay mode of the ttV background, with respect to the $m_{T2}^{ll}$ variable (in GeV), without applying any $m_{T2}^{ll}$ cut (left) and by applying the $m_{T2}^{ll} > 80$ GeV cut of our analysis (right). \\}
\label{ttV_plot}

\begin{itemize}
\item \textbf{Z$^{0}$ + jets}. The Drell-Yan process is the production of a Z boson in association with jets, and it is an important background of this analysis mainly because of its huge cross section. This background can be strongly reduced however by adding a Z-veto to our analysis (we reject all the events featuring either two electrons or two muons that have a reconstructed mass $m_{ll}$ closer than 15 GeV to the mass of the Z, equal to ($91,1876 \pm 0,0021$) GeV \cite{PDG_Z}). This cut, along with the introduction of the variable $m_{T2}^{ll}$, is quite powerful and removes a lot this background but, as previously stated, its huge cross section makes it impossible to get rid of this process completely. This background will be studied in more detail in Appendix  \ref{apen:DY}, since it is one of the two backgrounds which has been estimated using a data-driven method.
\item \textbf{Backgrounds with non-prompt leptons}. The calculations needed to study this background will be studied in Appendix \ref{apen:fakes}, since it is different in the sense that it can not actually be properly calculated using the usual Monte Carlo methods and has to be estimated using data-driven methods. Basically, it appears when a jet is misidentified as a lepton, and is an important background of this analysis. The best ways to reduce it strongly are to either apply a $p_T$ requirement on the leptons of the analysis (most of the non prompt leptons have a low $p_T$).
\item \textbf{Other backgrounds}. Other backgrounds appear in this analysis, but with the cuts we decided to apply, they almost do not affect the final results. We can cite for example the W$^{\pm}$W$^{\mp}$ process, which can produce one or several leptons, missing transverse energy and some jets, or the VZ$^{0}$ where the V can be either a W or a Z boson. We also looked at different processes having a Higgs in the final state but none of them seem to be able to survive to our selection.
\end{itemize}

%Event selection

\chapter{Event reconstruction and selection}

%The main objective of most of the analyses consists in plotting the data obtained with the LHC for different variables (such as the $p_T$ of the leptons or the missing transverse energy) in order to compare these distributions with respect to the ones obtained with Monte Carlo theoretical simulations for the different expected backgrounds of the analysis. Once this is done, the next step consists in looking for small excesses of data with respect to the simulations since, if we assume that the simulations are correct, this could be the sign that a process is missing. We can then add to the backgrounds distributions the expected distribution for the signal we are looking for, and check if the agreement between the data and the simulations improves. If it does and if the statistical significance of this agreement is better than 5 $\sigma$, a discovery can be claimed. This is basically how the Higgs boson was discovered in 2012 \cite{HiggsDiscovery1} \cite{HiggsDiscovery2}, and this is how we want to find dark matter as well. 

The techniques used to reconstruct the physics objects from the data collected by CMS will be detailed in this chapter, along with the method we developed to reconstruct and estimate the $p_T$ of the mediator of the dark matter production. Then, the variables which have a strong discriminating power for our analysis will be presented and our complete selection will be detailed. Finally, some control plots will be shown to check the validity of the simulation of different backgrounds, and a few words will be said about the systematics of the analysis.

\section{Event reconstruction} \label{sec:event_recon}

The objects we observe in the final state are reconstructed using different methods and algorithms contained in the CMSSW software package \cite{Previous}. The so-called Particule Flow algorithm introduced in Section \ref{subsubsec:CMS} allows us to reconstruct all the particles of the event, by combining the information coming from different parts of the detector (such as the tracks left by the charged particles, the energy deposited in the calorimeters and the impacts in the muon chambers). The primary vertex of the interaction is also reconstructed at this point, by calculating the sum of the square value of the $p_T$ of all the different vertices, the primary vertex being defined as the vertex with the largest value obtained.

\subsubsection*{Leptons}

Only the leptons having a $p_T$ higher than 10 GeV and $\eta$ values smaller than 2.5 (2.4) for electrons (muons) are used for this analysis. Following the recommendations of a CMS object group, and as explained in Section \ref{sec:objects}, both the electrons and muons are then selected by applying a tight identification requirement, based on the information coming from the tracker and from either the electromagnetic calorimeter (for electrons) or the muons chambers (for muons). Finally, a set of loose criteria is also defined to reject the events having more than two leptons and the leptons are required to be isolated from hadronic activity to be used in the analysis, to remove some of the contribution of the fake leptons, usually produced inside jets. This isolation is defined as the sum of the $p_T$ of the Particle Flow candidates within a cone size of $\Delta R = \sqrt{(\Delta \Phi)^2 + (\Delta \eta)^2} =$ 0.4.

\subsubsection*{Jets}

Jets are reconstructed from the Particle Flow candidates (excepting the charged candidates not coming directly from the primary vertex) with the anti-kT algorithm \cite{antiKT} within a cone $\Delta R$ equal to 0.4 and, to select the jet candidates, jet-area based corrections are applied in order to take into account the pile-up\footnote{Since the LHC is colliding bunches of protons, we observe many different events every time the beams cross. These multiple interactions happening every time are referred to as the \textit{pile-up}, and complicate the reconstruction of the event.}. The four momentum of the jets is also corrected by applying energy scale calibrations calculated from the data \cite{energy_scale_calib}. This analysis only considers jets having a $p_T$ larger than 30 GeV, a $\eta$ value smaller than 4 and passing a loose set of identification and isolation criteria (jets are required to be separated by more than a distance $\Delta R=0.3$ to any well-identified lepton). \\
 
 The CSVv2 algorithm \cite{b_tag} is used as b-tagger, to select jets coming from the hadronisation of a bottom quark. The medium working point of the tagger is used for the analysis, and leads to reconstruction efficiencies of the order of 69\% to tag b jets and to a mistagging rate of the order of 35 \% for the c jets and 1\% for the light flavour jets \cite{Previous}.

\subsubsection*{Missing transverse energy}

The missing transverse energy corresponds to the magnitude of the negative vectorial sum of the transverse momenta of all the particle candidates of the event. Type-I corrections (propagation of the jet energy corrections \cite{Type1_corr}) have been applied to remove events with large but artificial MET, according to the JET-MET POG recommendations \cite{MET_recom}. %The latest Jet Energy Corrections have also been propagated to the reconstruction of the $E_T^{\text{miss}}$.

\section{Top reconstruction} \label{sec:top_reco}

The top reconstruction is a method used to try to assign a $p_T$ value to the tops appearing in any $t \bar t$ like event. Getting this kind of information is really important for us because we want to be able to calculate the difference between the $p_T$ obtained for the top system and the potential dark matter, since this should give us a way to separate the usual $t \bar t$ process from the $t \bar t$ + DM process we are looking for. Indeed, in a pure $t \bar t$ event, we expect both the top quarks to leave the primary vertex almost back-to-back, while this shouldn't be the case in a $t \bar t$ + DM since we expect in this case that the difference in $p_T$ of the tops should be equal to the $p_T$ of the mediator produced at the same time. \\

If we leave the mediator aside for the moment and focus on the top reconstruction method applied to the usual $t \bar t$ process, we can realize quickly that reconstructing the $p_T$ of the tops is not an easy task, because we do not have any way to detect the neutrinos which appear once each one of the top decays (we actually have 6 unknowns, corresponding to the three components of the momenta of the two neutrinos produced). Fortunately, by studying the kinematics of the process, we can get to 6 different equations: \\

   \begin{minipage}[c]{.3\linewidth}
        \begin{equation*}
     	\begin{cases}
        		%M(b_1 + \nu_1 + l_1) = M_t \\
		%M(b_2 + \nu_2 + l_2) = M_t \\
		M(b_1 + W_1) = M_t \\
		M(b_2 + W_2) = M_t \\
     	\end{cases}
	\end{equation*}
	...because the tops produced immediately decay to a bottom and a W. \\
   \end{minipage} \hfill
   \begin{minipage}[c]{.3\linewidth}
   	\begin{equation*}
     	\begin{cases}
        		M(\nu_1 + l_1) = M_W \\
		M(\nu_2 + l_2) = M_W \\
     	\end{cases}
	\end{equation*}
	...because we consider the leptonic decay of the W in this analysis. \\
   \end{minipage} \hfill
   \begin{minipage}[c]{.3\linewidth}
   	\begin{equation*}
     	\begin{cases}
        		\nu_{1_x} + \nu_{2_x} = (E_T^{miss})_x \\
		%\nu_{1_y} + \nu_{2_y} = $\cancel{\textit{E}}$_{{T}_y} \\
		\nu_{1_y} + \nu_{2_y} = (E_T^{miss})_y \\
     	\end{cases}
	\end{equation*}
	...because we assume that the $E_T^{\text{miss}}$ is coming from the neutrinos only. \\
   \end{minipage} \hfill
   
\vspace{8pt}   
It is then possible to resolve this problem although it presents several technical complications. For example, the mass of the W and the mass of the top that appear in the previous equations can not be fixed, since they are distributes following a Breit-Wigner distribution and not constants. Moreover, the $E_T^{\text{miss}}$ is a variable that we can measure, but this measurement is usually affected by large uncertainties, and other sources of missing transverse energy could appear sometimes, making our calculation completely wrong since we assume that this $E_T^{\text{miss}}$ comes only from the two neutrinos produced. Finally, we do not actually know how to match each lepton with its corresponding b-jet, and we can also observe more than two jets in the same event, making it difficult to determine the right combination of jets to take for the calculation. All of these issues imply that the top reconstruction is not expected to work for every event and, sometimes, we just can not find any solution to the problem or even find many different solutions, each one corresponding to one combination of jets. \\

However, we do have some ways to reduce the impact of the previously mentioned issues, so that we can still manage to get some information with the top reconstruction. We actually just need to realize that the top reconstruction is not going to give us the exact momentum of the top, but will most likely return a probable value, which can be enough if we interpret the top reconstruction as a statistical problem. In practice, what we do is to consider all the (b-)jets of each event, and we then try to reconstruct the kinematics of the system by probing different combinations of leptons and jets. Then, we simply take the solution which gives the smallest mass for the top-antitop system. Applying this method to general $t \bar t$ events allows us to reconstruct around 99\% of the events with a 20\% resolution, but this reconstruction efficiency drops to around 50 \% (and these reconstructed events usually have a poor resolution) when we consider heavy $t \bar t$ + DM events. The reason for this efficiency drop and loss in resolution is simple: in the second case, the $E_T^{\text{miss}}$ is not only coming from the neutrinos, but also from the dark matter particles produced, making the previous equations wrong. The solutions we keep seeing are fortuitous, and appear when the $E_T^{\text{miss}}$ casually matches a combination of jets (along with their smearing) and W. However, in the dark matter case in particular, we expect the $p_T$ of the tops to be higher than the one obtained for the usual $t \bar t$ since they will be accommodating the extra $E_T^{\text{miss}}$. This is excellent news because if we manage to compensate the loss in efficiency, we should obtain an excellent variable to get some discrimination between these two processes. \\

What we have seen so far can be resumed in Figure \ref{fig:ellipse}. In this figure, we can see a purple ellipse corresponding to the phase space with solutions, and several points corresponding to the expected solutions for different kind of problems have been represented. As we can see in this figure and from the previous equations, the distance between the dark matter events without solution and the general phase space with solutions correspond directly to the $p_T$ of the mediator which, as we have seen at the beginning of this section, is a variable we are really interested in since we expect it to give a good discrimination between the usual $t \bar t$ and the $t \bar t$ + DM processes.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=11cm, height=6cm]{figs/ellipse.jpg}
\caption{Schematic view of the top reconstruction and the different expected solutions for different cases in the $E_T^{\text{miss}}$ phase space.}
\label{fig:ellipse}
\end{center}
\end{figure}

Then, the question remains to know if we can correct the efficiency drop or not, since throwing 50\% of the eventual DM events is of course not good. It is actually possible to correct this problem and to get an estimation of the $p_T$ of the mediator of the interaction at the same time. The solution of the previous equations gives a 4-degree polynomial and events with no solution have a polynomial which never crosses the x-axis. If we define the cost as the minimal distance between this polynomial and the x-axis, we can perform an iterative descent (as observed in Figure \ref{fig:descent}) of the observed $E_T^{\text{miss}}$ (blue dot in Figure \ref{fig:ellipse}) to reach the phase space corresponding to the physical solutions (purple ellipse in Figure \ref{fig:ellipse}), by using directly the definition of the derivative of the cost:

%Then, the question remains to know if we can correct the efficiency problem or not, since throwing 80\% of the eventual DM events is of course not a good way to start the analysis. It is actually possible to correct this problem and to get an estimation of the $p_T$ of the mediator of the interaction at the same time. The resolution of the previous equations actually gives a 4-degree polynomial and events with no solution have a polynomial which never crosses the x-axis. If we define the cost as the minimal distance between this polynomial and the x-axis, we can perform an iterative descent of the observed $E_T^{\text{miss}}$ (blue dot in Figure \ref{fig:ellipse}) to reach the phase space corresponding to the physical solutions (purple ellipse in Figure \ref{fig:ellipse}), by using directly the definition of the derivative of the cost:

\begin{equation}
	- \nabla \vec{cost} =
 	\begin{pmatrix}
    	\frac{\partial cost}{\partial MET_x} \\ \vspace{2pt}
    	\frac{\partial cost}{\partial MET_y} \\
	\end{pmatrix}
\end{equation}

   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 160pt]{figs/descent1.jpg}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 160pt]{figs/descent2.jpg}
	}
   \end{minipage} \hfill 
   \captionof{figure}{Iterative descent performed by minimizing the cost, in order to estimate the mediator $p_T$ as the distance between the solution found and the phase space with solutions, represented by the dark ellipse. \\}
   \label{fig:descent}

This method then allows us to correct for the efficiency issue of the top reconstruction by making most of the dark matter events converging points (for the heavy scalar mediator, we reach an efficiency of the order of 90\%, to be compared with the initial 50\%), and gives us moreover a way to estimate the $p_T$ of the mediator of the interaction (we assign to this new variable a negative value if the usual top reconstructions fails, and a value equal to 0 if our more developed reconstruction fails). This variable will be a key of our analysis, as we will now see in Section \ref{sec:discriminating_var}.

\section{Discriminating variables studies} \label{sec:discriminating_var}

Our complete analysis lays mostly on the discriminating power of four different variables that will be used as input for our neural network, as we will see in Chapter \ref{chap:MVA}, and which have been obtained by applying the first two levels of selection, as we will we in Table \ref{tab:our_cuts}. \\

The first of these variables is of course the missing transverse energy since, as previously explained, we expect to see $E_T^{\text{miss}}$ appearing with the $t \bar t$ + DM process. This variable is able to reduce some backgrounds, such as the DY, but as we can see in Figure \ref{plot:MET}, there is however not much we can do to reduce the $t \bar t$ or the $ttV$ for example with this variable only. \\

    \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 180pt]{figs/metPfType1.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 180pt]{figs/metPfType1_log.png} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Missing transverse energy distribution with (right) and without (left) logarithmic scale for the different processes of the analysis and three different scalar signal mass points, with the first level of event selection of Table \ref{tab:our_cuts} applied. The error bars represented are statistical only.\\} 
   \label{plot:MET}

\newpage

Another interesting variable is the angle $\Delta \Phi$ between the two leptons coming from the tops and the $E_T^{\text{miss}}$. We expect that the tops will be much closer to each other when the mass of the mediator produced raises (as seen in Figure \ref{fig:scheme_deltaphi}), and this variable should then theoretically give us a way to get some separation between the $t \bar t$ and the $t \bar t$ + DM. As we can see in Figure \ref{plot:DeltaPhi}, only a limited discrimination between some of the backgrounds and the signal is introduced in practice with this variable. \\

   \begin{minipage}[c]{.3\linewidth}
   	\centering{
		\includegraphics[width= 140pt, height= 140pt]{figs/deltaphi_mine.jpg}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.3\linewidth}
   	\centering{
		\includegraphics[width= 140pt, height= 140pt]{figs/deltaphi_low_mine.jpg} \\
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.3\linewidth}
   	\centering{
		\includegraphics[width= 140pt, height= 140pt]{figs/deltaphi_high_mine.jpg} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Schematic representation in the $\Phi$ plane of the distribution of the particles within the detector for the $t \bar t$ process (left) and the for the $t \bar t$ + DM (center and right). \\}
   \label{fig:scheme_deltaphi}
   
  \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 180pt]{figs/dphillmet.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 180pt]{figs/dphillmet_log.png} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Distribution of the $\Phi$ angle between the two leptons and the $E_T^{\text{miss}}$ with (right) and without (left) logarithmic scale for the different processes of the analysis and three different scalar signal mass points, with the first level of event selection of Table \ref{tab:our_cuts} applied. The error bars represented are statistical only. \\} 
   \label{plot:DeltaPhi}

The next variable, called $m_{T2}^{ll}$, is an interesting variable as well since it presents a large discriminating power, as we can see in Figure \ref{plot:Mt2ll}. The definition of this variable is not trivial but basically, it represents a measure of the imbalanced transverse momentum in an event and is mostly used for events for which two heavy particles are produced, each of which decaying to at least one undetected particle \cite{mt2ll, mt2ll3}. To calculate this variable, the following steps are needed.

\begin{itemize}
%\item First of all, we need to calculate the missing transverse energy and assign it to the two neutrinos produced.
\item We first need to calculate the transverse mass $M_T$ for the two pairs of particles produced, according to the Equation \ref{trans_mass}.
\begin{equation}
\left(M_T^{(i)}\right)^2 = \left(m_{vis}^{(i)}\right)^2 + m_{\chi}^2 + 2 \cdot \left(E_{T}^{vis(i)} E_{T}^{\chi(i)} - \overrightarrow{p}_{T}^{vis(i)} \cdot \overrightarrow{p}_{T}^{\chi(i)}\right)
\label{trans_mass}
\end{equation}

\newpage

\item However, there is a problem with the previous equation since the individual quantities $\overrightarrow{p}_{T}^{\chi(i)}$ of the neutrinos are not experimentally accessible (in this case, we can only determine the value of their sum, $\sum_{i}{\overrightarrow{p}_{T}^{\chi(i)}}$ = $\overrightarrow{E}_T^{\text{miss}}$). We can then generalize in the equation \ref{mt2} the definition of $M_T$ to a variable called $M_{T2}$, or equivalently $m_{T2}^{ll}$ in this work.
\begin{equation}
m_{T2}\left(m_\chi \right) = \min_{\sum_{i}{\overrightarrow{p}_{T}^{\chi(i)}}}\left[{\max{\left(M_T^{(1)}, M_T^{(2)}\right)}}\right]
\label{mt2} 
\end{equation}
\item The previous equation has a free parameter $m_{\chi}$ and the minimization is done over the different combinations of missing particles fulfilling the total $\overrightarrow{p}_{T}^{miss}$ constraint. For this analysis however, we make some assumptions to freeze this parameter. 
\end{itemize}

As we can see in Figure \ref{plot:Mt2ll}, adding a cut selecting only events with a large amount of $m_{T2}^{ll}$ seems like a good idea, since this removes a huge part of the $t \bar t$ background, the most problematic one, along with a good proportion of all the other backgrounds of the analysis (of course, we also throw away some signal but at the end of the day, we realized this cut removes the majority of the backgrounds).
%we expect the ratio $S/\sqrt{B}$ to improve by cutting in this variable (without the $m_{T2}^{ll} >$80 GeV cut, we actually observe a ratio of 2.96 for the 10 GeV scalar mediator signal sample, and this ratio becomes ... if we apply the cut of our analysis in this variable). \\

    \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 180pt]{figs/mt2ll.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 180pt]{figs/mt2ll_log.png} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Distribution of the $m_{T2}^{ll}$ variable with (right) and without (left) logarithmic scale for the different processes of the analysis and three different scalar signal mass points, with the first level of event selection of Table \ref{tab:our_cuts} applied. The error bars are statistical only. \\} 
   \label{plot:Mt2ll}

The last variable having a reasonable discriminating power is the so-called dark $p_T$, the $p_T$ of the mediator of the interaction obtained with the top reconstruction method developed in Section \ref{sec:top_reco}. \\

	\begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 180pt]{figs/darkpt.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 180pt]{figs/darkpt_log.png} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Distribution of the dark $p_T$ variable with (right) and without (left) logarithmic scale for the different processes of the analysis and three different scalar signal mass points, with the first level of event selection of Table \ref{tab:our_cuts} applied. The error bars are statistical only. \\} 
   \label{plot:Darkpt}

\section{Event selection} \label{sec:selection}

We will now discuss the selection and cuts we apply to our analysis, to remove as much background as we possibly can while leaving the signal as high as possible. The main objective of the cuts we apply is to improve the value of the significance, usually defined as the ratio between the number of yields of signal and the square root of the number of yields of all the backgrounds. A higher value of significance is of course what we are aiming for since it would mean that we managed to have more signal with respect to the backgrounds. \\

%We will now finally take some time to discuss the selection and cuts we apply to our analysis, to remove as much background as we possibly can while leaving the signal as high as possible. The main objective of the cuts we apply is to improve the value of the significance, usually defined as the ratio between the number of yields of signal and the square root of the number of yields of all the backgrounds, as shown in Equation \ref{eq:significance}. A higher value of significance is of course what we are aiming for since it would mean that we managed to have more signal with respect to the backgrounds. 

The cuts we apply in our analysis are described in Table \ref{tab:our_cuts}. Basically, we have applied two different levels of cuts, corresponding to a general prelesection and then to a selection more specific to our analysis. Here are some details about these two levels of cuts:
\begin{itemize}
\item First of all comes the preselection, an initial set of cuts that select only events passing the single or double lepton triggers listed in Section \ref{sec:triggers} and having at least two tight leptons with a $p_T$ higher than 25 GeV for the leading lepton (the one having the highest $p_T$) and higher than 20 GeV for the trailing lepton (the lepton having the second highest $p_T$). This is required to avoid the low $p_T$ regions where the triggers lose in efficiency and to remove some of the fake background contribution, since we expect the fake rate to be higher for low $p_T$ leptons. Moreover, we require these two leptons to have opposite charges. Finally, we also reject events having a significative third lepton ($p_T>$ 10 GeV) to remove most of the backgrounds featuring three or more leptons in the final state, such as the WZ.
\item Then come all the cuts more specific to our analysis in particular. For example, we require the invariant mass of the lepton pair $m_{ll}$ to be higher than 20 GeV in order to reject possible low mass resonances which are not taken into account by the simulated samples. We also apply a Z veto to the $ee$ and $\mu \mu$ channels in order to remove most of the Drell-Yan contribution, and we select only events having at least two jets ($p_T >$ 30 GeV) and at least one b-jet (medium working point of the CSSV v2 b-tagger). Then, we also look for events having a high $E_T^{\text{miss}}$ ($>$ 80 GeV) and a high value of $m_{T2}^{ll}$ since, as explained in Section \ref{sec:discriminating_var}, this variable has a strong discriminating power in our analysis. Finally, we ask the reconstructed mediator $p_T$ to be higher than 0, meaning that we select only events for which the top reconstruction didn't fail. \\
\end{itemize}

   \begin{center}
   %\resizebox{200pt} {!}{
   \begin{tabular}{c|c|c|c}
   Number & Cut level & Cut & Comment \\
   	& & & \\
   	\hline \hline
	& & & \\
	  \multirow{4}{*}{0} & \multirow{4}{*}{Preselection} & $p_{T}^{lep_1} >$ 25 GeV & On the leading lepton\\ 
	  & & $p_{T}^{lep_2}$ $>$ 20 GeV & On the trailing lepton \\ 
	  & & $p_{T}^{lep_3}$ $<$ 10 GeV &Third lepton veto \\ 
	  & & $q_{l}^{lep_1} \cdot q_{l}^{lep_2} < 0$ & Opposite charge requirement \\ 
	  & & & \\
	  \hline
	   & & & \\
	  \multirow{4}{*}{1} & \multirow{4}{*}{First level} & $m_{ll} > 20$ GeV &  \\
	  & & $|m_{ll} - m_Z| > 15$ GeV & Only applied to $ee$ and $\mu \mu$ channels \\
	  & & $n_{jet} \geq 2$ &  \\
	  & & $n_{b_{jet}} \geq 1$ & At least one medium csv-v2 b-jet \\
	  & & & \\
	  \hline
	  & & & \\
	  \multirow{3}{*}{2} & \multirow{3}{*}{Second level} & $E_T^{\text{miss}}$ $> 80$ GeV &  \\
	  & & $m_{T2}^{ll} >$ 80 GeV &  \\
	  & &  Dark $p_T>$ 0 GeV & The top reconstruction has to work \\
	  & & & \\
   \end{tabular}
   \captionof{table}{Description of the complete selection applied to our analysis.}
   \label{tab:our_cuts}
   %} 
   \end{center}

%The most important distributions that we will use in this work have been represented in Figure \ref{fig:preSel}. These distributions have been obtained at the first level of selection, and have the $t \bar t$ and DY scale factors calculated in Section \ref{sec:tt_ScaleF} and appendix \ref{apen:DY} applied. \\

% \begin{minipage}[c]{.48\linewidth}
 %  	\centering{
%		\includegraphics[width= 180pt, height= 160pt]{figs/metPfType1_log-preSel.png}
%	}
   %\end{minipage} \hfill
   %\begin{minipage}[c]{.48\linewidth}
   	%\centering{
	%	\includegraphics[width= 180pt, height= 160pt]{figs/dphillmet_log-preSel.png} \\
	%}
   %\end{minipage} \hfill
   
   %\vspace{10pt}
   
   %\begin{minipage}[c]{.48\linewidth}
   	%\centering{
	%	\includegraphics[width= 180pt, height= 160pt]{figs/mt2ll_log-preSel.png} \\
	%}
   %\end{minipage} \hfill
   %\begin{minipage}[c]{.48\linewidth}
   	%\centering{
	%	\includegraphics[width= 180pt, height= 160pt]{figs/darkpt_log-preSel.png} \\
	%}
   %\end{minipage} \hfill
   %\captionof{figure}{Most important discriminating variables of the analysis with only the most basic cuts of the analysis applied. The error bars represented are statistical only.}
   %\label{fig:preSel}

%\section{Signal extraction}\label{sec:signalextraction}
%Harder for low masses than high masses

\section{Control regions}

\subsection{$t \bar t$ scale factor and control region} \label{sec:tt_ScaleF}

Since the $t \bar t$ is our most important background, we want to make sure that we understand it right and that the simulations are correct. One way to check this consists in calculating a scale factor, defined as \ref{eq:tt_SF}, in bins of the variable $m_{T2}^{ll}$, by which we can scale the MC simulations.

\begin{equation}
SF = \frac{\left(n_{\mbox{data}} - n_{\mbox{other backgrounds}}\right)}{n_{t \bar t}}
\label{eq:tt_SF}
\end{equation}

The scale factor dependance with $m_{T2}^{ll}$ obtained this way can be represented in Figure \ref{fig:tt_SF} in which the red line corresponds to a fitted constant to the points in the region where $m_{T2}^{ll}$ is comprised between 40 and 80 GeV (this plot has been obtained by applying the cuts corresponding to the first level of selection, along with the 40 $< m_{T2}^{ll} < $ 80 GeV and dark $p_T \geq$ 0 cuts). We obtain a value of ($0.974 \pm 0.003$) for this scale factor, where the uncertainty shown correspond to the statistical error only.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width= 8cm, height= 7cm]{figs/ttSF-running_short.png}
\caption{Calculated scale factor for the $t \bar t$ process in bins of $m_{T2}^{ll}$. The error bars represented are statistical only.}
\label{fig:tt_SF}
\end{center}
\end{figure}

Let's now start studying the $t \bar t$ control region, in order to check if the major background of the analysis seems to be correctly simulated. This control region is defined with the same cuts as the ones applied to our analysis (as seen in Table \ref{tab:our_cuts} in Section \ref{sec:selection}) except that we remove the $E_T^{\text{miss}} > 80$ GeV requirement, and that we switch the $m_{T2}^{ll} >  80$ GeV to $m_{T2}^{ll} < 80$ GeV, to get a region as pure in $t \bar t$ as possible. The distributions for three different variables ($E_T^{\text{miss}}$, $\Delta \phi_{ll, E_T^{\text{miss}}}$ y $m_{T2}^{ll}$) are represented in Figure \ref{fig:tt_CR}, where the $t \bar t$ and DY scale factors calculated in Section \ref{sec:tt_ScaleF} and appendix \ref{apen:DY} have been applied. As we can see, in the region defined this way, we obtain 85,7\% of $t \bar t$ and we can observe a nice agreement between the data and the MC. \\% (of the order of 3\%). \\

   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 150pt, height= 158pt]{figs/metPfType1_ttCR.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 150pt, height= 158pt]{figs/dphillmet_ttCR.png} \\
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 150pt, height= 158pt]{figs/mt2ll_ttCR.png} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Some distributions in the control region defined to check for the validity of the $t \bar t$ process.}
   \label{fig:tt_CR}

\subsection{ttZ control region}

We can perform a similar analysis to check for the validity of the ttZ process. To get a control region as pure as possible in ttZ, we only look at events having three leptons in the final state with one pair having a mass close to the one expected for the Z in order to remove most of the backgrounds, such as $t \bar t$. We also require the $E_T^{\text{miss}}$ to be higher than 50 GeV and we only select events having $m_{T2}^{ll} > 80$ GeV and at least one b-jet. The distributions for three variables of the analysis are represented in Figure \ref{fig:ttV_CR}, where we see that we obtain 50\% of ttZ and a scale factor of ($1.85 \pm 0.29$).  \\

%Error = sqrt(data)/ttZ

\begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 150pt, height= 158pt]{figs/metPfType1_ttVCRv2.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 150pt, height= 158pt]{figs/dphillmet_ttVCRv2.png} \\
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 150pt, height= 158pt]{figs/mt2ll_ttVCRv2.png} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Some distributions in the control region, to check for the validity of the ttZ.}
   \label{fig:ttV_CR}

%\subsection{Same sign control region}

%This control region has been defined simply by reversing the opposite sign requirement of our analysis (as we will see in Section \ref{sec:selection}, one of the the first cut we apply consists in asking for two leptons having opposite charges to remove some backgrounds, such as the Drell-Yan) to now require two same sign leptons. This defines a control region in which we expect the non prompt and ttV backgrounds to be dominant, since only a small amount of backgrounds can produce two same sign leptons. Some distributions obtained for the $E_T^{\text{miss}}$ variable are shown in Figure \ref{plot:SS_control}. \\

%\begin{center}
%\color{red} Add plots with a similar selection to the analysis \color{black}
%\end{center}

\section{Uncertainties of the analysis}

All the measurements we can make in high energy physics and in physics in general present some uncertainties, and the determination of their value is a critical point of every analysis, especially in this case since we try to detect a really low signal over a large background. A detailed study about the origin and the impact on the final results of all the systematic uncertainties being well beyond the scope of this work, we will only consider and make some general comments about the statistical and some systematic uncertainties. \\

Statistical uncertainties are easy to deal with, and appear in any counting experiment \cite{Statistics} since each measurement, made of a finite set of observations, results in different observations. The statistical uncertainty is then just a measure giving us an idea about the range of this variation. What we do is that we consider that our counting experiment can be approximated with a Poisson distribution (we actually have a binomial distribution but this approximation is justified whenever the number of measurements $N$ gets really high, as this is usually the case in particle physics since the number of bunch crossings and the number of particles per bunch are really high, or when the probability of observing an event gets really small), for which we know that the error on the number of measurements is directly given by its square root. \\

The systematic uncertainties are just as important in this analysis, but they are more difficult to estimate and are really different in nature \cite{Systematics} since they arise directly from the theory or from the detector itself. A typical example of systematic uncertainty might be related to the calibration of the detector, since a bad calibration will have the same impact on all the measurements we want to perform. This kind of uncertainty can be separated into two different categories usually referred to as the theoretical and the experimental systematic uncertainties.

\begin{itemize}
\item \textbf{Theoretical systematic uncertainties}. This category is related directly to the theoretical models we use in the analysis and is mostly related to the production of the Monte Carlo simulations, since we use them to predict the background. Indeed, this kind of simulation lays on several different models which are not able to describe nature in a perfect way. For example, as accurate as the Feynman diagrams are, it is impossible for us to make them represent the complete picture since they get really complicated once we start considering next to leading order (NLO), or even next to next to leading order (NNLO) perturbations. The number of possible diagramas increases exponentially with each level of precision, and so do the computational needs to take them all into account. Usually, the Monte Carlo simulations are performed at NLO order, which might introduce a small systematic uncertainty. There also exists another uncertainty of this kind arising from the fact that we do not know the exact shape of the Parton Density Functions (PDF), a crucial element when it comes to the production of theoretical simulations.
\item \textbf{Experimental systematic uncertainties}. Many different sources of experimental systematics have been studied in this work, to get results as precise as possible. We can separate this kind of uncertainties itself into two different categories (the same distinction can also be made for the theoretical systematics).
	\begin{itemize}
	\item[$\circ$] \textbf{Scale uncertainties}. To calculate their impact, we observe the change in yields obtained when moving the nominal values up and down by a value given by this uncertainty. The 2.5\% error on the determination of the integrated luminosity \cite{lumi} is a typical example which is treated this way. The lepton reconstruction and identification efficiencies, the triggers acceptance, the b-tagging efficiency or the fake rate uncertainties all belong to this category as well \cite{AN-16-005}.
	\item[$\circ$] \textbf{Shape uncertainties}. Sometimes, the systematic uncertainty can not just be resumed to a single number or percentage, but is determined by the complete shape of a variable. The jet energy scale is a typical example corresponding to this category \cite{AN-16-005}.
	\end{itemize}
\end{itemize}

As previously explained, understanding and taking into account all the different sources of systematic uncertainties is crucial in this analysis, especially when producing the final limits that will be shown in Section \ref{sec:results}. Indeed, we need to get a precise idea about the error we are committing on the different number of yields we measure, in order to be able to detect eventual significative deviation between the expected and measured limits for the different mediator masses considered.

%Neural network

\chapter{Neural network}\label{chap:MVA}

Performing a general cut and count analysis is usually not optimal when looking for dark matter production, since the significance of this signal is expected to be really low, mostly because of the different backgrounds such as the $t \bar t$. Because of this low significance, we need to come up with solutions to extract as much information as we can from the data. This is the main reason why we decided to perform a Multi-Variate Analysis (MVA) by creating several Artifical Neural Networks (ANN), using general machine learning techniques. First of all, we will study a bit of theory concerning this kind of analysis, to understand better the reasons why we decided to use it and the precautions we took using it. Then, we will talk about and study different parameters of the neural networks we have built in particular, and we will finally show all the results we obtained.

\section{Multi-Variate Analysis}

As the name of this method \cite{MVA} suggests, the main idea of this kind of analysis consists in observing several different variables at the same time, to combine the information coming from all of them in order to find a way to classify a single event as either background or signal. We want with this method to be able to reduce a large number of inputs to a single output and the MVA is performed in this case using several neural networks, as we will see in Section \ref{sec:ANN}. In summary, the objective of the MVA is to use and feed this network with the input information coming from the four variables having the most discriminating power in our analysis (as described in Section \ref{sec:discriminating_var}), to get in return a new single variable that we can use in our analysis to discriminate the signal and the backgrounds. 

\section{Artificial Neural Network} \label{sec:ANN}

A general Artificial Neural Network has been represented in Figure \ref{fig:ANN_schema}, where we can see that it is able to combine different input variables into a single one, through a hidden layer made out of neurons. Each neuron of this layer is able to combine the different inputs it receives in a certain way, through the definition of several weights $\omega_i$, which are numbers expressing the importance of the respective inputs to the output of the neuron \cite{ANN}. \\

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=8cm, height=5cm]{figs/MVA_schema.jpg}
\caption{General representation of the ANN defined for the MVA analysis, able to combine the discriminating power of several different input variables into a single one \cite{MVA}.}
\label{fig:ANN_schema}
\end{center}
\end{figure}

In this work, we will consider networks made out of two different kinds of neurons having either a sigmoid or a tangent hyperbolic activation function. The output of a sigmoid neuron is a real number between 0 and 1, and can be defined with the sigmoid activation function \ref{eq:sigmoid}, where $x_j$ are the inputs of the neuron, $\omega_j$ the weights and $b$ the bias, defined as a measure of how easy it is to get the neuron to give an output different than 0 \cite{ANN_functions}.

\begin{equation}
\sigma(z) = \frac{1}{1+e^{-z}} = \frac{1}{1+e^{-\sum_j{\omega_j x_j} -b}}
\label{eq:sigmoid}
\end{equation}

\newpage
%\vspace{5pt}
On the other side, the tanh activation function \ref{eq:tanh} has a slightly different expression, giving back a number between -1 and 1 (but for this analysis, we will actually convert back this output to get a number between 0 and 1 as well). As in the previous equation, $z$ is defined as $\sum_j{\omega_j x_j} -b$.

\begin{equation}
tanh(z) = \frac{e^z - e^{-z}}{e^z+e^{-z}}
\label{eq:tanh}
\end{equation}

\vspace{5pt}
An important point to note is that a neural network in itself is useless, since we first need to train it with a fraction of the events we have at our disposition for it to be able to calculate the output and to therefore create a new variable useful to distinguish between signal and background. We have to be careful however when choosing the number of training events, because a number too small could result in a inadequate fit because of the loss of fitting capacity of the network. We could then be tempted to use as many events as possible to train our network to improve it, so that we can get a more reliable output, but we actually have to be careful not to use too many events for training, for two main reasons. First of all, simply because the events we use to train the network are actually lost, since we can not use them later on for the analysis and then because we have to be really careful to avoid the overtraining issue of the network. \\

This overtraining appears when the network is creating a perfect model fitting the training events instead of trying to find a way to generalize the trend observed with this dataset. A simple example of this overtraining issue can be made by considering a network allowed to create a model which has a number of parameters greater than the number of observations. In this case, the network will give us back a model fitting perfectly the training data since it is always possible to find a function with $n$ degrees of freedom passing exactly through $n$ points, but such a model will typically fail drastically when making predictions about new data because of the loss of generalization. It will then be crucial to check for any sign of overtraining with the networks that will be used in this analysis.

\section{Characterization of the neural networks}

\subsection{Input variables}

The four variables used as input for our neural networks are the variables ($E_T^{\text{miss}}$, $\Delta \phi_{ll, E_T^{\text{miss}}}$, $m_{T2}^{ll}$ and the so-called dark $p_T$, the reconstructed $p_T$ of the mediator) presented in Section \ref{sec:discriminating_var}, because they are the variables offering the highest discriminating power between background and signal. The different distributions of the input variables used for the MVA have been represented in Figure \ref{fig:MVAInput} thanks to the TMVA (Toolkit for Multivariate Analysis) tool \cite{TMVA}, for both the 10 GeV and 500 GeV scalar mediators. It is important to note at this point that we train a different neural network for each mass point. \\

   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width=200pt, height=192pt]{figs/newMVA/input_10GeV.pdf}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 192pt]{figs/newMVA/input_500GeV.pdf} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Distribution of the signal and background for the four input variables used for the MVA. The background in red corresponds to the $t \bar t$ process, while the signal in blue corresponds to either the 10 GeV scalar mediator, the mass point offering the worst discrimination (left) or the 500 GeV scalar mediator, offering the best discrimination (right). \\}
   \label{fig:MVAInput}
   
We can also plot in Figure \ref{fig:correlation} the correlation matrices for the background and signal, for the four different variables considered. This is useful mainly because we want to use the smallest possible number of input variables to feed our network in order to simplify the analysis, and this plot allows us to check for eventual correlations and therefore detect eventual superfluous input variables. \\

\begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 170pt, height= 167pt]{figs/newMVA/correlation_background_10GeV.pdf}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 170pt, height= 167pt]{figs/newMVA/correlation_signal_10GeV.pdf} \\
	}
   \end{minipage} \hfill
   
   \vspace{4pt}
   
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 170pt, height= 167pt]{figs/newMVA/correlation_background_500GeV.pdf}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 170pt, height= 167pt]{figs/newMVA/correlation_signal_500GeV.pdf} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Study of the correlation between the input variables, for both the 10 GeV (top) and 500 GeV (bottom) scalar mediators, considering the background (left) and the signal (right). \\}
   \label{fig:correlation}

By looking at the previous plots, we can conclude that we do not observe any strong correlation (or anti-correlation) for the background, if we consider the input variables of the neural networks built for the 10 and 500 GeV mediator masses. The 10 GeV signal distributions do not present any sign of strong correlation either, but we can see that three of the four input variables used to build the 500 GeV network actually present some correlation between each other. 

\subsection{Architecture}

The architecture of a neural network used in this analysis is represented in Figure \ref{fig:ANN}. As we can see, the neural networks we built are made out of two hidden layers, containing respectively six and three neurons. They are able to read the input from the four previous variables and give us in return a single variable which combines the information from all the input variables. We consider in this work both sigmoid-like and tanh-like neurons, to compare the results we obtain in both cases. \\

    \begin{minipage}[c]{.98\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 140pt]{figs/newMVA/sigmoid_architecture.pdf}
	}
   \end{minipage} \hfill
   \captionof{figure}{Architecture of the sigmoid neural network built for the 10 GeV signal sample of this analysis. This network has two layers of hidden neurons and consider four input variables.}
    \label{fig:ANN}

\subsection{Training}

In order to get an efficient neural network, we need to train it properly first. The network is for the moment trained only against the $t \bar t$, the most important background in our analysis, in order to avoid overtraining issues and to simplify the analysis. We use 200 events to train both the networks to avoid any eventual overtraining, and as previously explained, we train a different network for each dark matter mediator mass, and for each mediator type. \\

The relative importance of the different input variables for the training can be calculated automatically \cite{TMVA} as the sum of the weights-squared of the connections between the input variables and the first layer of the network, as described in Equation \ref{eq:importance}, where $I_i$ corresponds to the importance and where $\bar x_i$ is the mean value of the variable $i$ considered.

\begin{equation}
I_i = \bar x_i^2 \sum_{j=1}^{n_h}{\left(\omega_{ij}^{(1)}\right)^2}
\label{eq:importance}
\end{equation}

\vspace{5pt}
The results obtained for the importance of our variables are represented in Table \ref{tab:importance}, for the two different networks. As we can see in this table, the importance of the variable changes quite a lot depending on the signal sample used to train the ANN: when we consider the 10 GeV nework, we see that the most useful variable is our dark $p_T$ but when we consider the 500 GeV network, the most useful variable is $m_{T2}^{ll}$ for both the sigmoid and tanh-like cases.  Moreover, we can also see that the $\Delta \phi_{ll, E_T^{\text{miss}}}$ variable is clearly the less important variable in all the cases and finally, we can conclude that both the sigmoid and tanh-like networks are giving similar results, even though the ranking of importance of some variables change from time to time.
   
  	\begin{center}
	\begin{tabular}{c|c|c|c|c}
		& \multicolumn{2}{c|}{\textbf{10 GeV network}} & \multicolumn{2}{c}{\textbf{500 GeV network}} \\
		Variable & Sigmoid-like & Tanh-like & Sigmoid-like & Tanh-like \\
		 & & & & \\
		\hline \hline
		& & & & \\
		$m_{T2}^{ll}$ & 7.18 & 3.70 & 17.75 & 14.83 \\
		dark $p_T$ & 11.06 & 10.93 & 4.62 & 6.64 \\
		$E_T^{\text{miss}}$ & 3.84 & 4.48 & 6.77 & 5.69 \\
		$\Delta \phi_{ll, E_T^{\text{miss}}}$ & 0.58 & 0.82 & 0.40 & 0.69 \\
		& & & & \\
	\end{tabular}
	\end{center}
   
   \captionof{table}{Importance of the different variables used for the training of the neural networks for both the sigmoid-like (left) and tanh-like networks (right).}
      \label{tab:importance}

\section{Results}

Before checking the actual output we get, we need to check some control plots to verify that everything went well and to compare the different networks created. First of all, we can plot the convergence test in Figure \ref{fig:convergence}, to check the convergence in all the cases, for both the training and test samples. As we can see in this figure, in the 500 GeV case, both the training and test samples converge at the same value, while they seem to be converging at different values in the 10 GeV case, which is the typical sign that we are committing some overtraining in this case. \\

   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 190pt, height= 170pt]{figs/newMVA/convergence_sigmoid_10GeV.pdf}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 190pt, height= 170pt]{figs/newMVA/convergence_tanh_10GeV.pdf} \\
	}
   \end{minipage} \hfill
   
   \vspace{5pt}
   
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 190pt, height= 170pt]{figs/newMVA/convergence_sigmoid_500GeV.pdf}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 190pt, height= 170pt]{figs/newMVA/convergence_tanh_500GeV.pdf} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Study of the convergence for the training and test samples, for both the 10 GeV (top) and 500 GeV (bottom) networks, and for both the sigmoid (left) and tanh cases (right). \\}
   \label{fig:convergence}
   
   \newpage

At this point, it is time to check in Figure \ref{fig:overtraining} for eventual signs of overtraining in the response obtained by any of the networks. As we can see in all the plots, we do not observe any sign of overtraining since the distributions obtained for the test and train samples are quite similar for both the signal and the background. The Kolmogorov-Smirnov test seems to indicate however that the sigmoid response is a bit better and presents less overtraining, for both the 10 and 500 GeV networks. We can also clearly see that the 10 GeV signal sample presents distributions much closer to the background distributions, while the 500 GeV signal gets a much better separation with the background. \\

   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 190pt, height= 170pt]{figs/newMVA/response_sigmoid_10GeV.pdf}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 190pt, height= 170pt]{figs/newMVA/response_tanh_10GeV.pdf}
	}
   \end{minipage} \hfill
   
   \vspace{5pt}
   
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 190pt, height= 170pt]{figs/newMVA/response_sigmoid_500GeV.pdf}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 190pt, height= 170pt]{figs/newMVA/response_tanh_500GeV.pdf}
	}
   \end{minipage} \hfill
   \captionof{figure}{Study of the response of the network, to search for eventual overtraining, for both the 10 GeV (top) and 500 GeV (bottom) networks, and for both the sigmoid (left) and tanh cases (right). \\}
   \label{fig:overtraining}

We can also study the background rejection versus the signal efficiency in both cases, as represented in Figure \ref{fig:MVAROC} for the 10 GeV and 500 GeV dark matter scalar mediators. This is an important plot to consider, because the output given by any of the networks is used for the moment in a simple cut and count analysis, by finding the optimal significance point using this significance curve. As we can see in this figure, we actually won't be able to find an optimal cut for the analysis, and it will always be a compromise. We can either try to raise the signal efficiency, but this will let pass trough more background, or try to reject as much background as possible while losing in signal efficiency at the same time. However, as expected, we can reach a much better signal efficiency in the 500 GeV case than the one possible to obtain for the 10 GeV mediator, by keeping a similar level of background rejection. \\
   
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 190pt, height= 168pt]{figs/newMVA/ROC_10GeV.pdf}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 190pt, height= 168pt]{figs/newMVA/ROC_500GeV.pdf}
	}
   \end{minipage} \hfill
   \captionof{figure}{Comparison of the background rejection with respect to the signal efficiency of our MVA, for both the 10 (left) and 500 GeV (right) neural networks. \\}
\label{fig:MVAROC}

Finally, we can plot the significance curves and distributions obtained for the output of the sigmoid-like networks we have built in Figures \ref{fig:final_MVAROC} and \ref{fig:final_MVA}. With all the cuts of the analysis applied, we do not observe a strong discrimination with the new variable coming from the network for the 10 GeV mass point, but we do get a much more significative discrimination for the 500 GeV scalar mediator (we observe a clear maximum in the red curve around 0.98 in this latest case). \\

	\begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 180pt, height= 162pt]{figs/ANN_sigm_mt2ll80_Cedrictest_ttDM0001scalar00010_significance.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 180pt, height= 162pt]{figs/ANN_sigm_mt2ll80_Cedrictest_ttDM0001scalar00500_significance.png} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Significance curves of the new variable obtained, for both the 10 GeV (left) and 500 GeV (right) mediators. Errors are statistical only and all the cuts of the analysis have been applied. \\}
   \label{fig:final_MVAROC}

   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 180pt, height= 162pt]{figs/ANN_sigm_mt2ll80_Cedrictest_ttDM0001scalar00010_log.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 180pt, height= 162pt]{figs/ANN_sigm_mt2ll80_Cedrictest_ttDM0001scalar00500_log.png} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Distributions of the new variable obtained, for both the 10 GeV (left) and 500 GeV (right) mediators. Errors are statistical only and all the cuts of the analysis have been applied.  \\}
   \label{fig:final_MVA}

%Results obtained

\chapter{Results} \label{sec:results}

First of all, in Table \ref{tab:yields_final} are represented the expected yields for the main backgrounds, different signal masses and for the data at the final selection level, corresponding to an integrated luminosity 2.4 fb$^{-1}$ and for the three different channels available separately and then combined. 

\begin{center}
	\begin{tabular}{c|c|c|c|c}
		Process & Channel $ee$ & Channel $e \mu$ & Channel $\mu \mu$ & Channel $ll$ \\
		& & & \\
		\hline \hline
		& & & \\
		WW &    0.02 $\pm$    0.01 &    0.06 $\pm$    0.03 &    0.20 $\pm$    0.05 &    0.28 $\pm$    0.06 \\ 
		WZ &    0.02 $\pm$    0.01 &    0.01 $\pm$    0.01 &    0.01 $\pm$    0.01 &    0.04 $\pm$    0.01 \\ 
		VZ &    0.00 $\pm$    0.00 &    0.01 $\pm$    0.00 &    0.00 $\pm$    0.00 &    0.01 $\pm$    0.00 \\ 
		VVV &    0.00 $\pm$    0.00 &    0.01 $\pm$    0.00 &    0.02 $\pm$    0.01 &    0.03 $\pm$    0.01 \\ 
		Z+jets &    0.59 $\pm$    0.35 &    1.14 $\pm$    0.53 &    0.00 $\pm$    0.00 &    1.73 $\pm$    0.63 \\ 
		$t \bar t$V &    0.32 $\pm$    0.02 &    0.66 $\pm$    0.04 &    1.09 $\pm$    0.04 &    2.07 $\pm$    0.06 \\ 
		$t \bar t$ &   46.95 $\pm$    0.33 &  119.20 $\pm$    0.56 &  191.34 $\pm$    0.69 &  357.49 $\pm$    0.95 \\ 
		tW &    1.64 $\pm$    0.14 &    4.01 $\pm$    0.22 &    6.78 $\pm$    0.28 &   12.43 $\pm$    0.39 \\ 
		Non prompt &    0.00 $\pm$    1.64 &    0.00 $\pm$    1.32 &   11.74 $\pm$    3.07 &   11.70 $\pm$    3.72 \\
		& & & \\
		\hline 
		& & & \\
		\textbf{Scalar mediator} & & & \\
		m$_{\chi}$ 1, m$_{\phi}$ 10 (x10) &   25.07 $\pm$    1.57 &   58.42 $\pm$    2.56 &   93.76 $\pm$    3.12 &  177.25 $\pm$    4.33 \\ 
		m$_{\chi}$ 1, m$_{\phi}$ 100 (x100) & 25.45 $\pm$    0.95 &   67.24 $\pm$    1.66 &  106.83 $\pm$    2.02 &  199.53 $\pm$    2.78 \\  
m$_{\chi}$ 1, m$_{\phi}$ 500 (x10$^{3}$) &    4.95 $\pm$    0.11 &   12.35 $\pm$    0.19 &   18.66 $\pm$    0.23 &   35.96 $\pm$    0.32 \\
		& & & \\
		\textbf{Pseudoscalar mediator} & & & \\
		m$_{\chi}$ 1, m$_{\phi}$ 10 (x10) &  1.96 $\pm$    0.06 &    5.08 $\pm$    0.10 &    7.92 $\pm$    0.12 &   14.95 $\pm$    0.17 \\ 
		m$_{\chi}$ 1, m$_{\phi}$ 100 (x100) & 12.34 $\pm$    0.35 &   31.61 $\pm$    0.60 &   47.42 $\pm$    0.72 &   91.38 $\pm$    1.00 \\ 
m$_{\chi}$ 1, m$_{\phi}$ 500 (x10$^{3}$) &   5.00 $\pm$    0.12 &   13.65 $\pm$    0.21 &   20.13 $\pm$    0.25 &   38.78 $\pm$    0.34 \\
		& & & \\
		\hline 
		& & & \\
		Total background &   49.53 $\pm$    1.71 &  125.10 $\pm$    1.55 &  211.17 $\pm$    3.16 &  385.77 $\pm$    3.91 \\
		& & & \\
		\hline
		& & & \\
		Data &   51.00 &  133.00 &  211.00 &  395.00 \\
		& & & \\
	\end{tabular}
	\captionof{table}{Expected yields for the main backgrounds and some signals of the analysis at the final selection level, and measured yields in data. The V boson can either stand for a W or a Z boson, and the errors are statistical only.}
	\label{tab:yields_final}
\end{center}

\newpage 

We can then plot in Figure \ref{fig:final_results} all the main variables of the analysis at the final selection level, for three different scalar mediator masses (10, 100 and 500 GeV). The same is done for the different pseudoscalar mediators in Figure \ref{fig:final_results_pseudo}. \\

Using the data available we can finally also establish an upper-limit on the dark matter production cross section value for different mediator masses. Plotting the upper limit on the cross section at the 95\% confidence level allows us to eventually reject some of the dark matter mass points considered. Indeed, if we get a value smaller than one for the limit, we would expect to have enough sensitivity and therefore to be able to detect the model considered. \\

The limits obtained for different mediator masses and for scalar mediators are shown in Table \ref{tab:limits}. These limits correspond to the best limits obtained by changing the value of the cut applied on the output variable coming using a tanh-like neural network, as previously described in Chapter \ref{chap:MVA}. The best output of the MVA is for now considered as the best significance point, but we will move to a complete shape analysis as soon as possible.

	%\begin{minipage}[c]{.48\linewidth}
	\begin{center}
	\resizebox{480pt} {!}{
   	\begin{tabular}{c|c|c|c|c|c}
		& & & & & \\
		$m_S$ [GeV]& Best AAN cut & Expected central limit & 1$\sigma$ interval & 2$\sigma$ interval & Observed limit \\ 
		& & & & & \\ 
		\hline \hline
		& & & & & \\
		10 & 0.75 & 2.99 & [1.95, 4.87] & [1.38, 7.81] & 2.22 \\
            	20 & 0.80 & 3.12 & [2.04, 5.04] & [1.43, 8.14] & 2.73 \\
            	50 & 0.90 & 4.14 & [2.66, 6.81] & [1.87, 11.19] & 2.90 \\
            	100 & 0.90 & 7.03 & [4.43, 11.80] & [3.05, 19.81] & 5.94 \\
            	200 & 0.90 & 25.37 & [16.26, 41.36] & [11.40, 67.93] & 19.69 \\
            	300 & 0.80 & 54.25 & [35.20, 88.42] & [24.69, 144.04] & 50.34 \\
            	500 & 0.98 & 175.50 & [106.94, 308.41] & [71.64, 531.91] & 116.65 \\
		& & & & & \\
          \end{tabular}
          }
          \end{center}
   %\end{minipage} \hfill
   
   \captionof{table}{Limits obtained at an integrated luminosity of 2.4 fb$^{-1}$, for different masses corresponding to the scalar mediators. \\}
	\label{tab:limits}
   
   The same can of course be repeated, considering this time different masses of pseudoscalar mediators. These results can be found in Table \ref{tab:limits_pseudo}.
   
   %\begin{minipage}[c]{.48\linewidth}
   	\begin{center}
	\resizebox{480pt} {!}{
   	\begin{tabular}{c|c|c|c|c|c}
		& & & & & \\
		$m_S$ [GeV]& Best ANN cut & Expected central limit & 1$\sigma$ interval & 2$\sigma$ interval & Observed limit \\ 
		& & & & & \\ 
		\hline \hline
		& & & & &  \\
		10 & 0.80 & 9.47 & [6.17, 15.28] & [4.38, 24.46] & 7.66 \\
            	20 & 0.85 & 9.94 & [6.50, 16.04] & [4.60, 25.90] & 8.18 \\
            	50 & 0.85 & 11.47 & [7.50, 18.51] & [5.31, 29.63] & 10.18 \\
            	100 & 0.90 &11.03 & [7.05, 18.33] & [4.93, 30.09] & 8.91 \\
            	200 & 0.95 & 17.31 & [10.77, 29.32] & [7.41, 49.18] & 9.81 \\
            	300 & & & & & \\
            	500 & 0.98 & 167.94 & [103.19, 289.77] & [69.86, 495.14] & 105.27 \\
		& & & & & \\
          \end{tabular}
         }
          \end{center}
   %\end{minipage} \hfill
          
          \captionof{table}{Limits obtained at an integrated luminosity of 2.4 fb$^{-1}$, for different masses corresponding to the pseudoscalar mediators. \\}
	\label{tab:limits_pseudo}
	
	We can now plot these expected and observed limits in Figure \ref{fig:limits} along with the 1 and 2 $\sigma$ intervals (in green and yellow, respectively), for the different mass points considered and for both the scalar and the pseudoscalar mediators. 
	
	\begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 210pt, height= 200pt]{figs/limits_scalar.pdf}
		%\color{red} Scalar limits plot coming soon! \color{black}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 210pt, height= 200pt]{figs/limits_pseudo.pdf}
		%\color{red} Pseudoscalar limits plot coming soon! \color{black}
	}
   \end{minipage} \hfill
   \captionof{figure} {Limits obtained for 2.4 fb$^{-1}$ for our analysis, for both the scalar (right) and pseudoscalar mediators (left). \\}
   \label{fig:limits}	
	
	As we can see in the previous tables and plots, we can not exclude any dark matter model by applying our analysis to the blinded dataset corresponding to an integrated luminosity of 2.4 fb$^{-1}$. Since we know that the expected limit is being reduced as the square root of the luminosity, we expect to be able to exclude the 10 and 20 GeV scalar mediators when unblinding our analysis. All the expected limits for the full 2016 dataset calculated this way can be found in Table \ref{tab:expected_36_limits}. \\
	
	\begin{center}

    \begin{minipage}[c]{.48\linewidth}
    	\begin{center}
   	\begin{tabular}{c|c}
		$m_S$ [GeV]& Expected limit  \\ 
		& \\ 
		\hline \hline
		& \\
		10 &  0.77\\
            	20 &  0.81\\
            	50 & 1.07 \\
            	100 & 1.82  \\
            	200 & 6.55 \\
            	300 & 14.00\\
            	500 & 45.31 \\
		& \\
          \end{tabular}
          \end{center}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\begin{center}
   	\begin{tabular}{c|c}
		$m_S$ [GeV]& Expected limit  \\ 
		& \\ 
		\hline \hline
		& \\
		10 & 2.44 \\
            	20 & 2.57  \\
            	50 & 2.96  \\
            	100 & 2.84 \\
            	200 & 4.47 \\
            	300 &  \\
            	500 & 43.36 \\
		& \\
          \end{tabular}
          \end{center}
   \end{minipage} \hfill
   
	\captionof{table}{Expected limits for 35.9 fb$^{-1}$ for the scalar (left) and pseudoscalar (right) mediators.}
	\label{tab:expected_36_limits}
	
\end{center}

\newpage

   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/lep1pt_log-final.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/lep1eta_log-final.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/m2l_log-final.png}
	}
   \end{minipage} \hfill
   
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/lep2pt_log-final.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/lep2eta_log-final.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/mt2ll_log-final.png}
	}
   \end{minipage} \hfill

   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/darkpt_log-final.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/dphillmet_log-final.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/metPfType1_log-final.png}
	}
   \end{minipage} \hfill 
   
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/njet_log-final.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/nbjet30csvv2m_log-final.png}
	}
   \end{minipage} \hfill
    \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/ht_log-final.png}
	}
   \end{minipage} \hfill
   
   \captionof{figure}{Final distributions for different variables at the final selection level of the analysis, for the data, the backgrounds and three different scalar mediator masses. All the signals have been rescaled by a factor given in the legend of the plots, and the errors are statistical only.\\}
   \label{fig:final_results}
   
   \newpage

   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/lep1pt_log-final_pseudo.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/lep1eta_log-final_pseudo.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/m2l_log-final_pseudo.png}
	}
   \end{minipage} \hfill
   
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/lep2pt_log-final_pseudo.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/lep2eta_log-final_pseudo.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/mt2ll_log-final_pseudo.png}
	}
   \end{minipage} \hfill

   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/darkpt_log-final_pseudo.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/dphillmet_log-final_pseudo.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/metPfType1_log-final_pseudo.png}
	}
   \end{minipage} \hfill 
   
    \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/njet_log-final_pseudo.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/nbjet30csvv2m_log-final_pseudo.png}
	}
   \end{minipage} \hfill
    \begin{minipage}[c]{.32\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 150pt]{figs/ht_log-final_pseudo.png}
	}
   \end{minipage} \hfill
   
   \captionof{figure}{Final distributions for different variables at the final selection level of the analysis, for the data, the backgrounds and three different pseudoscalar mediator masses. All the signals have been rescaled by a factor given in the legend of the plots, and the errors are statistical only. \\}
   \label{fig:final_results_pseudo}

%Conclusions

\chapter{Conclusions}

A search for dark matter production is association with a pair of top quarks in the dilepton final state has been developed throughout this work. The analysis has been made using a 2.4 fb$^{-1}$ dataset for the signal region and the complete 35.9 fb$^{-1}$ dataset for the control regions, both taken by the CMS detector at CERN, at a center of mass energy of $\sqrt{s} = 13$ TeV.  \\

After a general and a theoretical introduction about the process studied and the different channels of production of dark matter at the LHC, we studied the different objects, datasets and triggers used for the analysis. We also studied in detail the different backgrounds of the analysis and the ways we have to estimate them and check their validity. 
%In particular, the determination of one of these backgrounds, the non prompt leptons, using a data-driven method has been developed a bit more in Appendix \ref{apen:fakes} and different control regions have been plotted to check for the validity of the different Monte Carlo simulations. 
We then saw some generalities about the top reconstruction method we developed in order to be able to estimate the $p_T$ of the mediator of the interaction studied, and the procedure deployed to correct for the low efficiency observed using a generic top reconstruction method. We also described the four variables able to introduce some discrimination between the major background $t \bar t$ of the analysis and the signal, we detailed the cuts we apply to the events to get a sample as pure as possible in $t \bar t$ + DM signal and different control regions have been plotted to check for the validity of the different Monte Carlo simulations. We also explained some theory concerning the Multi-Variate Analysis we decided to apply to this analysis by developing different kinds of Artificial Neural Networks, in order to create a new variable allowing us to separate the backgrounds from the signal. Finally, we showed some distributions and yields obtained for the different processes at the final selection level, and we managed to obtain an upper limit on the cross section, considering different masses for both the scalar and pseudoscalar mediators. \\

As explained in Chapter 7, the results with the unblinded dataset do not allow to exclude any dark matter mediator production model so far. There are however several ways to improve this analysis in the near future. First of all, the increase in luminosity given by the unblinding of the complete 2016 dataset and the new collisions happening daily at the LHC are expected to lower the limits obtained by a factor proportional to the square root of the change in integrated luminosity, as we saw in Table \ref{tab:expected_36_limits}. Moreover, a better understanding of the different backgrounds of the analysis (especially the $t \bar t$, of course) and the different systematics is also a crucial point of the analysis, since studying them in detail and reducing them would also allow us to improve our results. Finally, moving to a shape study instead of a simple cut and count analysis on the MVA output is expected to improve a lot the final results obtained and will be done as soon as possible.

\begin{appendices}
  
  \chapter{Data-driven methods}\label{apen:data-driven}
  
  The Monte Carlo simulations might not describe properly all the effects that happens in a collider, since they are extremely complex to produce and depend on a lot of different theoretical parameters. Another way to study the different backgrounds is to calculate the yields we expect, directly from the data we measured. This is a difficult task as well and has been done so far to characterize two different important processes: the Drell-Yan and the non-prompt leptons. Each method of calculation will now be explained in detail.
  
  \section{Rin-out method for the Drell-Yan process}\label{apen:DY}
  
  This method has been developed in order to find a way to calculate directly from the data a factor by which we can scale the Drell-Yan (DY) produced thanks to the usual MC simulations \cite{Brochero}. This is an important process because it has a huge cross section (as seen in Table \ref{tab:MC_samples}) and because it is expected to contribute to the three different channels of the analysis ($e^+ e^-$, $\mu^+ \mu^-$ and $e^{\pm} \mu^{\mp}$ via tau decays), so we want to make sure that we understand it well enough. The main idea of this method consists in considering the number of DY events in MC inside ($N_{DY}^{in}$) and outside ($N_{DY}^{out}$) of the Z mass window (defined as the region where the reconstructed mass of the two leptons takes values between 76 and 106 GeV) with all the cuts of the final selection level of the  Section \ref{sec:selection} applied, except for the Z veto.

\vspace{5pt}
We can define the ratio $R_{out/in}$ as being the ratio between the number of DY events predicted outside and inside of the Z mass window by the simulations. This ratio can then be used to calculate the number of DY events expected outside of the Z window in data, from the number of events observed within this window, according to the following equation: 

\begin{equation}
N_{DY, est}^{out} = N_{DY, data}^{in} \cdot \left( \frac{N_{DY, MC}^{out}}{N_{DY, MC}^{in}} \right) =  N_{DY, data}^{in} \cdot R_{out/in}
\end{equation}

\vspace{5pt}
It is important to note that the previous equation assumes that the Z window is dominated by DY events, which is not an evident assumption to make, since different backgrounds should be present in this window as well (peaking backgrounds such as ZZ and WZ and non peaking backgrounds that we actually do not consider in this calculation since they give a continuous distribution in the dilepton invariant mass). The number of yields within the peak of the Z can then be described with the equation \ref{DY_data}, where $k_{ll}$ is a factor applied to opposite flavor final states to normalize for the relative efficiencies for electrons and muons.

\begin{equation}
N_{DY, est}^{out} = (N_{ll, data}^{in} - k_{ll} \cdot N_{e \mu, data}^{in} - N_{ZV, MC}^{in}) \cdot R_{out/in}
\label{DY_data}
\end{equation}

\vspace{5pt}
It is then possible to show that the scale factor for the channel $ee$ takes the form \ref{SF_DY} (a similar equation can of course be derived in the $\mu \mu$ case).

%\begin{equation*}
%	 \begin{cases}
%	        \vspace{5pt}
%		k_{ee} = \frac{1}{2} \cdot \sqrt{\frac{n_{ee}^{obs}}{n_{\mu \mu}^{obs}}} \\
%		SF_{ee} = \frac{n_{ee}^{in} - n_{WZ}^{in} - n_{ZZ}^{in} - k_{ee} \cdot n_{e \mu}^{in}}{n_{DY}^{in}} \\
%	\end{cases}	
%\end{equation*}

\begin{equation}
	SF_{ee} = \frac{n_{ee}^{in} - n_{WZ}^{in} - n_{ZZ}^{in} - k_{ee} \cdot n_{e \mu}^{in}}{n_{DY}^{in}} \mbox{  where  } k_{ee} = \frac{1}{2} \cdot \sqrt{\frac{n_{ee}^{obs}}{n_{\mu \mu}^{obs}}}
\label{SF_DY}
\end{equation}

\vspace{5pt}
The scale factors obtained for each channel and for different bins of $m_{T2}^{ll}$ are shown in Figure \ref{fig:DYscale}. \\ 

   %\begin{minipage}[c]{.48\linewidth}
   	%\centering{
	%	\includegraphics[width= 200pt, height= 200pt]{figs/DYscale_met.pdf}
	%}
   %\end{minipage} \hfill
   %\begin{minipage}[c]{.48\linewidth}
   	%\centering{
	\begin{center}
		\includegraphics[width= 200pt, height= 200pt]{figs/dy_scale_mt2ll.png} \\
	\end{center}
	%}
   %\end{minipage} \hfill
   \captionof{figure}{Scale factor obtained from the Rin-out data-driven method for the Drell-Yan process, represented with respect to bins of $m_{T2}^{ll}$, for the three channels of our analysis (all the possible combinations of flavors of two leptons). The error bars represented are statistical only.}
   \label{fig:DYscale}
  
  \section{Fake and prompt rates}\label{apen:fakes}
  
  We can start this section by giving some useful definitions. First, a \textit{prompt lepton} is defined as a real lepton, in the sense that the lepton is originating from the primary interaction vertex of a collision while on the other hand, a \textit{non-prompt} or \textit{fake lepton} is defined as a lepton falsely detected (usually, this kind of lepton comes from a mis-identified jet or from a heavy meson decay). The \textit{fake rate} corresponds to the probability to see a fake lepton passing the final selection level of the analysis, giving rise to a new kind of background, especially at low lepton $p_T$. This new background source therefore needs to be understood quite well, the problem being that this misidentification rate can not be properly estimated by Monte Carlo simulations. This is why this background is estimated using data-driven methods. In this work, the fake rate is estimated within a loose single lepton triggered sample and is calculated separately for electrons and muons. It is important to note at this point that even though this background is small in this analysis, the method presented here is a general method which can be used in most of the analyses, since it does not depend on the number of final-state particles or on the event selection applied. \\

The fundamental idea of the method is quite simple. We first want to define a control region as pure as possible, in which we can estimate the yields of the QCD backgrounds, featuring a lot of jets in the final state. Then, we just want to calculate directly from the data an extrapolation factor allowing us to go back to our signal region where we perform the analysis. The measurement of the fake rate is made by defining the so-called \textit{fakeable objects}, which are lepton-like objects passing only the loose requirements, and the \textit{fully selected objects}, which correspond to objects able to pass the complete selection of our analysis. The fake rate is then easily calculated as the ratio between the number of fully reconstructed leptons and the number of fakeable objects. \\

We have now seen how it is possible to calculate the fake rate, but we are not quite done yet since an important step is still missing. As previously stated, for this method to be useful, we have to define a QCD-enriched control region, but we also expect this control region to have some contamination, coming mainly from the leptonic decay of the eventual W or Z bosons that survived our selection. This kind of contamination is usually referred to as the electroweak contamination and is considered in this work to be coming from the Z+jets and W+jets processes only. It is necessary to take into account this contamination because it biases the number of fully reconstructed leptons which appear in the fake rate definition, leading to a wrong fake rate calculation, especially at high $p_T$, where the contribution of real leptons is higher. This contamination has been removed in this analysis. The fake rates obtained using this method, with and without electroweak correction, are represented in Figure \ref{fig:fr}, for both electrons and muons. \\

\begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 175pt]{figs/Ele_FR_pt_35GeV.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 175pt]{figs/Ele_FR_eta_35GeV.png} \\
	}
   \end{minipage} \hfill
   
  \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 175pt]{figs/Muon_FR_pt_25GeV.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 175pt]{figs/Muon_FR_eta_25GeV.png} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Fake rates obtained for both electrons (top) and muons (bottom), with respect to the $p_T$ (left) and $\eta$ (right) with 35.9 fb$^{-1}$ of integrated luminosity. \\}
   \label{fig:fr}

The fake rate is not the only important rate to calculate, since we also need to estimate the \textit{prompt rate} to take into account the real lepton contamination in the control region we defined. This rate is also measured in data, using a general tag and probe method within a Z enriched control region. This method consists simply in reconstructing Z $\rightarrow$ ll events in this region, and to select all the events for which the first lepton can be characterized as tight. Then, we search for the second lepton coming from the Z decay within all the leptons detected, by calculating the reconstructed mass of all the possible combinations and selecting the one which is closer to the expected mass of the Z. If we find a pair of leptons giving a reconstructed mass close to the expected one, and if we know that one of the lepton is tight,  we can conclude that the second lepton should be tight as well, and we can look if this is effectively the case or not. This allows us to calculate the electron and muon prompt rates, which are represented in Figure \ref{fig:pr}. \\

\begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 175pt]{figs/Ele_PR_pt.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 175pt]{figs/Ele_PR_eta.png} \\
	}
   \end{minipage} \hfill
   
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 175pt]{figs/Muon_PR_pt.png}
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.48\linewidth}
   	\centering{
		\includegraphics[width= 200pt, height= 175pt]{figs/Muon_PR_eta.png} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Prompt rates obtained for both electrons (top) and muons (bottom), with respect to the $p_T$ (left) and $\eta$ (right) with 35.9 fb$^{-1}$ of integrated luminosity. \\}
   \label{fig:pr}

%Once the fake and prompt rates computed, and bidimensional histograms representing the fake and prompt rates in function of the $p_T$ and $\eta$ created, our work is not over yet. We now need to find a way to go back to our signal region, to estimate the proportion of this new background in our analysis region, by calculating the different probabilities associated to the rates previously calculated. 
Once the fake and prompt rates computed, the final step then consists in defining a same sign control region where we expect this background to be dominant, to check it with respect to the data directly. The same sign plots obtained this way are represented in Figure \ref{fig:SS_control}. \\

%\begin{equation*}
%    \begin{cases}
%        PF = \mathcal{P}($\text{prompt}$) \cdot \mathcal{P}($\text{fake}$) \\
%        FP = \mathcal{P}($\text{fake}$) \cdot \mathcal{P}($\text{prompt}$) \\
%        FF = \mathcal{P}($\text{fake}$) \cdot \mathcal{P}($\text{fake}$)
%     \end{cases}
%\end{equation*}\\

%This simple set of equations allows us to calculate the weight to apply to the data (in the dilepton case) to calculate the yields corresponding to this new background. The last step consists in defining a same sign control region where we expect this background to be high to check it with respect to the data directly, as we will see in Section \ref{subsec:control}. \\

\begin{minipage}[c]{.3\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 170pt]{figs/cratio_ee0j_met.png} \\
	}
   \end{minipage} \hfill
   \begin{minipage}[c]{.3\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 170pt]{figs/cratio_mm0j_met.png} \\
	}
   \end{minipage} \hfill
    \begin{minipage}[c]{.3\linewidth}
   	\centering{
		\includegraphics[width= 160pt, height= 170pt]{figs/cratio_of1j_met.png} \\
	}
   \end{minipage} \hfill
   \captionof{figure}{Same sign control plots for the $ee$ 0 jet bin channel (on the left), the $\mu \mu$ 0 jet bin channel (on the middle) and the different flavor 1 jet bin channel (on the right). The error bars represented are statistical only.}
   \label{fig:SS_control}
  
   \chapter{Personal work and framework used}
  
  The almost two years spent working at the IFCA were a great opportunity for me to learn how to use different frameworks in both C++ and python, and how to work in a large collaboration with other people. In the physics point of view, I also learnt a lot about particle physics in general and in particular about the methods used to make precision measurements (I first measured the cross section production of the well-known WZ process), to estimate some backgrounds using data-driven methods (I spent quite some time studying how to estimate the non-prompt background which appear in most of the analyses performed) and to search for new exotic particles. \\
  
The large majority of the plots appearing in this work are mine and have been made by either completely writing the scripts from scratch, or by slightly modifying scripts which had previously been written by the IFCA or the so-called \textit{latino} collaboration.
  
\end{appendices}

\addcontentsline{toc}{chapter}{Bibliography}

\begin{thebibliography}{1}

\bibitem{QFT} \textsc{M. Born}, \textsc{P. Jordan} \& \textsc{W. Heisenberg}, \textit{Zur Quantenmechanik. II.}, Physik, 1926 \\ 
\url{http://link.springer.com/article/10.1007\%2FBF01379806}

\bibitem{Quarks} \textsc{M. Riordan}, \textit{The Discovery of Quarks}, SLAC-PUB-5724, April 1992 \\ 
\url{http://www-spires.slac.stanford.edu/cgi-wrap/getdoc/slac-pub-5724.pdf}

\bibitem{HiggsPostulate}\textsc{F. Englert} \& \textsc{R. Brout}, Phys. Rev. Lett. \textbf{13}, 321 (1964); \\
                  \textsc{P.W. Higgs}, Phys. Rev. Lett. \textbf{13}, 508 (1964) and Phys. Rev. \textbf{145}, 1156 (1966); \\
                  \textsc{G.S. Guralnik, C.R. Hagen} \& \textsc{T.W. Kibble}, Phy. Rev. Lett. \textbf{13}, 585 (1964).

\bibitem{HiggsDiscovery1} \textsc{CMS Collaboration}, \textit{Observation of a new boson at a mass of 125 GeV with the CMS experiment at the LHC}, Phys. Lett. B, September 2012 [arXiv:1207.7235] \\ 
\url{http://www.sciencedirect.com/science/article/pii/S0370269312008581}

\bibitem{HiggsDiscovery2} \textsc{ATLAS Collaboration}, \textit{Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC}, Phys. Lett. B, September 2012 [arXiv:1207.7214] \\ 
\url{http://www.sciencedirect.com/science/article/pii/S037026931200857X}

\bibitem{SM_wiki} \textit{Standard Model}, Wikipedia, as seen in May 2017 \\ 
\url{https://en.wikipedia.org/wiki/Standard_Model}

\bibitem{Repartition}\textit{Dark Matter}, CERN, as seen in May 2017 \\ 
\url{https://home.cern/about/physics/dark-matter}
          
 \bibitem{Newton} \textsc{I. Newton}, \textit{Newton's Demonstration that planets move in ellipses},
Philosophical Transactions of the Royal Society, No. 128, p. 698-705, September 1676 \\ \url{http://www.newtonproject.ox.ac.uk/view/texts/diplomatic/NATP00092}      
          
\bibitem{Zwicky} \textsc{F. Zwicky}, \textit{Die Rotverschiebung von extragalaktischen Nebeln},
 Helvetica Physica Acta, Vol. 6, p. 110-127, 1933 \\ \url{http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1933AcHPh...6..110Z&amp;data_type=PDF_HIGH&amp;whole_paper=YES&amp;type=PRINTER&amp;filetype=.pdf}
 
 \bibitem{Zwicky_off} \textsc{S. Van Den Bergh}, \textit{The early history of dark matter},
Dominion Astrophysical Observatory, June 1999 \\ \url{https://arxiv.org/pdf/astro-ph/9904251.pdf}
 
 \bibitem{Curves} \textsc{K.G. Begeman}, \textsc{A.H. Broeils} \& \textsc{R.H. Sanders}, \textit{Extended rotation curves of spiral galaxies - Dark haloes and modified dynamics}, Monthly Notices of the Royal Astronomical Society (ISSN 0035-8711), April 1991 \\ \url{http://adsabs.harvard.edu/full/1991MNRAS.249..523B}
 
 \bibitem{MOND} \textsc{R. Scarpa}, \textit{Modified Newtonian Dynamics, an Introductory Review}, European Southern Observatory, September 2005 \\ \url{https://cds.cern.ch/record/923578/files/0601478.pdf} 
 
  \bibitem{sky_surveys} \textit{Galaxy Clusters Reveal New Dark Matter Insights}, NASA Jet Propulsion Laboratory, 2016 \\ \url{https://www.jpl.nasa.gov/news/news.php?feature=4829} 
  
   \bibitem{lensing} \textsc{T. Treu} \& \textsc{E. Koopmans}, \textit{Probing dark matter distribution with gravitational lensing and stellar dynamics}, California Institute of Technology, May 2002 [arXiv:0205335] \\ \url{https://arxiv.org/abs/astro-ph/0205335} 
 
  \bibitem{MACHO} \textsc{K. Griest}, \textit{WIMPs and MACHOs}, Encyclopedia of astronomy and astrophysics, 2002 \\ \url{http://www.astro.caltech.edu/~george/ay20/eaa-wimps-machos.pdf} 
 
 \bibitem{ParticlePhysics} \textsc{M. Peskin}, \textit{Dark Matter and Particle Physics}, SLAC-PUB-12493, July 2007 [arXiv:0707.1536]  \\ 
\url{https://arxiv.org/pdf/0707.1536.pdf}

 \bibitem{Production} \textsc{S. Yan Hoh}, \textsc{J. R. Komaragiri} \& \textsc{W. A. T. Wan Abdullah}, \textit{Dark Matter Searches at the Large Hadron Collider}, SLAC-PUB-12493, 2016 [arXiv:1512.07376] \\ 
\url{https://arxiv.org/pdf/1512.07376.pdf}

\bibitem{Blog} \textsc{M. Buckley} \textit{Paper Explainer: Two is not always better than one: Single Top Quarks and Dark Matter}, PhysicsMatt \\ \url{http://www.physicsmatt.com/blog/2017/1/21/paper-explainer-two-is-not-always-better-than-one-single-top-quarks-and-dark-matter}

\bibitem{Top_mass} \textsc{D $\Phi$ collaboration}, \textit{A precision measurement of the mass of the top quark}, Nature 429, p. 638-642, June 2004 \\ \url{http://www.nature.com/nature/journal/v429/n6992/full/nature02589.html}

\bibitem{Top} \textsc{T. M. Liss} \& \textsc{P. L. Tipton}, \textit{The Discovery of the Top Quark}, Ecole Polytechnique Federale de Lausanne, September 1997 \\ \url{http://lphe.epfl.ch/~mtran/seminaires/Cours_Master_Bordeaux/Articles/SciAmTop.pdf}
          
\bibitem{Previous} \textsc{CMS Collaboration}, \textit{Search for dark matter in association with a top quark pair at $\sqrt{s} = 13$ TeV in the dilepton channel}, CMS-PAS-EXO-16-028, 2016.          
          
\bibitem{Goals} \textit{Sidebar: 5 Goals for the LHC}, Scientific American, as seen in May 2017 \\ \url{https://www.scientificamerican.com/article/5-goals-for-the-lhc/}           

\bibitem{LHC} \textit{Particle Accelerators and Detectors}, Universe Review \\ \url{https://universe-review.ca/R15-20-accelerators03.htm} 
          
\bibitem{Plan} \textit{LHC roadmap} \\ \url{https://lhc-commissioning.web.cern.ch/lhc-commissioning/schedule/LHC\%20schedule\%20beyond\%20LS1\%20MTP\%202015_Freddy_June2015.pdf}, as seen in April 2017.          
          
 \bibitem{CMS_general} \textit{CMS}, CERN, as seen in May 2017. \\ \url{https://home.cern/fr/about/experiments/cms}           
          
\bibitem{CMSScheme} \textit{CMS detector design}, CERN \\ \url{http://cms.web.cern.ch/news/cms-detector-design} 

\bibitem{CMSInside} \textit{Compact Muon Solenoid: Wikis}, The Full Wiki \\ \url{http://www.thefullwiki.org/Compact_Muon_Solenoid}, as seen in April 2017.

\bibitem{PF} \textsc{CMS Collaboration} , \textit{Particle-Flow Event Reconstruction in CMS and Performance for Jets, Taus and MET}, Technical Report CMS-PAS-PFT-09-001, 2009 \\ \url{https://cds.cern.ch/record/1194487?ln=fr}

%\bibitem{AN-16-172} \textsc{CMS Collaboration}, \textit{Update on common analysis object definitions and trigger efficiencies for the H to WW Run-2 analysis}, CMS-AN-16-172, 2017.

\bibitem{lumi} \textsc{CMS Collaboration}, \textit{CMS Luminosity Measurements for the 2016 Data Taking Period}, CMS-PAS-LUM-17-001, 2017.

\bibitem{blinding} \textsc{A. Rao} , \textit{Blinding and unbliding analyses}, CERN news, June 2012 \\ \url{http://cms.web.cern.ch/news/blinding-and-unblinding-analyses}, as seen in May 2017.

\bibitem{Runs} \textit{PdmV2016Analysis}, CERN twiki \\ \url{https://twiki.cern.ch/twiki/bin/view/CMS/PdmV2016Analysis}, as seen in May 2017.

\bibitem{Brilcalc} \textit{BRIL Work Suite}, CMS service lumi \\ \url{https://cms-service-lumi.web.cern.ch/cms-service-lumi/brilwsdoc.html}, as seen in May 2017.

\bibitem{MC_general} \textsc{P. Nason} \& \textsc{P.Z. Skands}, \textit{Monte Carlo events generators}, Particle Data Group, September 2013 \\ \url{http://pdg.lbl.gov/2014/reviews/rpp2014-rev-mc-event-gen.pdf}

\bibitem{Powhegv2} \textsc{C. Oleari}, \textit{The POWHEG BOX}, Nuclear Physics B Proceedings Supplements 205, August 2010 [arXiv:1007.3893] \\ \url{https://arxiv.org/pdf/1007.3893.pdf}

\bibitem{Pythia8} \textsc{T. Sjöstrand}, \textsc{P. Skands} \& \textsc{S. Prestel} , \textit{PYTHIA 8 Worksheet}, Lund University, January 2014 \\ \url{http://home.thep.lu.se/~torbjorn/pythia81html/Welcome.html}

\bibitem{Geant4} \textit{Geant4}, CERN, May 2017\url{http://geant4.cern.ch}

%\bibitem{Our} \textsc{A. Calderon et al.}, \textit{Search for dark matter production in association with top quark pairs in the dilepton final state at $\sqrt{s}$ = 13 TeV}, CMS-AN-16-478, May 2017.

\bibitem{DMForum} \textsc{D. Abercrombie et al.} , \textit{Dark Matter Benchmark Models for Early LHC Run-2 Searches: Report of the ATLAS/CMS Dark Matter Forum}, July 2015 [arXiv:1507.00966] \\ \url{https://arxiv.org/pdf/1507.00966.pdf}

\bibitem{Xs} \textsc{CERN Twiki} , \textit{Summary table of samples produced for the 1 Billion campaign, with 25ns bunch-crossing} \\ \url{https://twiki.cern.ch/twiki/bin/viewauth/CMS/SummaryTable1G25ns#TTbar}, as seen in April 2017.

\bibitem{tt} \textsc{B. Yang} \& \textsc{N. Liu}, \textit{One-loop QCD correction to top pair production in the littlest Higgs model with T-parity at the LHC}, EPL, 2015 [arXiv:1507.07104] \\ 
\url{https://arxiv.org/abs/1507.07104}

\bibitem{PDG_Z} \textsc{J. Beringer et al.}, \textit{Z mass}, Particle Data Group, PR D86, 2012 \\ \url{http://pdg.lbl.gov/2012/listings/rpp2012-list-z-boson.pdf}

\bibitem{antiKT} \textsc{M. Cacciari}, \textsc{G. P. Salam} \& \textsc{G. Soyez}, \textit{The anti-kt jet clustering algorithm}, JHEP, 2008 [arXiv:0802.1189] \\ \url{https://arxiv.org/abs/0802.1189}

\bibitem{energy_scale_calib} \textsc{CMS Collaboration}, \textit{Determination of Jet Energy Calibration and Transverse Momentum Resolution in CMS}, JINST 6, 2011 [arXiv:1107.4277] \\ \url{http://cds.cern.ch/record/2138504}

\bibitem{b_tag} \textsc{CMS Collaboration}, \textit{Identification of b quark jets at the CMS Experiment in the LHC Run 2}, Technical Report CMS-PAS-BTV-15-001, CERN, 2016 \\ \url{https://arxiv.org/abs/1107.4277}

\bibitem{Type1_corr} \textit{MET Analysis}, CERN Twiki, June 2015 \\ \url{https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookMetAnalysis#Type_I_Correction}, as seen in May 2017.

\bibitem{MET_recom} \textsc{JET-MET POG}, \textit{MET Filter Recommendations for Run II}, CERN Twiki \\ \url{https://twiki.cern.ch/twiki/bin/viewauth/CMS/MissingETOptionalFiltersRun2}, as seen in May 2017.

\bibitem{mt2ll} \textsc{C. Jones}, \textit{Using the $M_{T2}$ and $\Phi^*$ variables to investigate Monte Carlo methods of $t \bar t$ production}, September 2015 \\ \url{http://www.desy.de/2011summerstudents/2015/reports/CaitlinJones.pdf}, as seen in May 2017

\bibitem{mt2ll3} \textsc{CMS Collaboration}, \textit{Searches for supersymmetry using the MT2 variable in hadronic events produced in pp collisions at 8 TeV}, May 2015 [arXiv:1502.04358] \\ \url{https://arxiv.org/abs/1502.04358}

\bibitem{Statistics} \textsc{T. R. Junk}, \textit{Statistical Methods for Experimental Particle Physics}, July 2009 \\ \url{https://www-cdf.fnal.gov/~trj/tsi09/trjtsi_Day1.pdf}

\bibitem{Systematics} \textsc{P. K. Sinervo}, \textit{Definition and Treatment of Systematic Uncertainties in High Energy Physics and Astrophysics}, Toronto University, September 2003\\ \url{http://hep.physics.utoronto.ca/~pekka/papers/systematicsreview.pdf}

\bibitem{AN-16-005} \textsc{CMS Collaboration}, \textit{Search for dark matter in association with a top quark pair at $\sqrt{s}$ = 13 TeV}, CMS PAS-EXO-16-005, August 2016 \\ \url{https://cds.cern.ch/record/2204933}

\bibitem{MVA} \textsc{M. Jachowski}, \textit{Multivariate Analysis, TMVA, and Artificial Neural Networks}, Michigan REU Final Presentations, August 2006 \\ \url{https://indico.cern.ch/event/5007/contributions/1177811/attachments/962648/1366777/reu_presentation3.pdf}

\bibitem{ANN} \textsc{M. A. Nielsen}, \textit{Using neural nets to recognize handwritten digits}, Determination Press, Chapter 1, 2015 \\ \url{http://neuralnetworksanddeeplearning.com/chap1.html}

\bibitem{ANN_functions}  \textit{Neural Networks}, Stanford University, April 2013 \\ \url{http://ufldl.stanford.edu/wiki/index.php/Neural_Networks}

\bibitem{TMVA} \textsc{A. Hoecker}, \textsc{P. Speckmayer}, \textsc{J. Stelzer}, \textsc{J. Therhaag}, \textsc{E. von Toerne}, \textsc{H. Voss} \& \textsc{M. A. Nielsen} \textit{Toolkit for Multivariate Data Analysis with ROOT: Users Guide}, Stanford University, September 2013 \\ \url{http://tmva.sourceforge.net/docu/TMVAUsersGuide.pdf}

\bibitem{Brochero} \textsc{J. Brochero}, \textit{Top Anti-Top Production Cross Section Measurement at $\sqrt{s}$ = 8 TeV in the Dilepton Channel with the CMS Detector}, CERN-THESIS-2014-209, July 2014 \\ \url{http://digital.csic.es/bitstream/10261/141369/1/Tesisdetectorbrochero.pdf}
          
\end{thebibliography}

\end{document}
